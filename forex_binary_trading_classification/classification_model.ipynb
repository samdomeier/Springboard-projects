{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import modf\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# graphing modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_data/updated_apicall_data\\ext_AUDNZD_train_3_min_pos.csv (2174, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_AUDUSD_train_3_min_pos.csv (2636, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_EURCAD_train_3_min_pos.csv (2624, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_EURGBP_train_3_min_pos.csv (1728, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_EURJPY_train_3_min_pos.csv (7864, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_GBPAUD_train_3_min_pos.csv (5565, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_GBPCHF_train_3_min_pos.csv (1562, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_GBPUSD_train_3_min_pos.csv (1306, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_USDAUD_train_3_min_pos.csv (6180, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_USDCAD_train_3_min_pos.csv (5923, 64)\n",
      "../data/full_data/updated_apicall_data\\ext_USDJPY_train_3_min_pos.csv (4235, 64)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/full_data/updated_apicall_data/*.csv'\n",
    "for file in glob.glob(data_path):\n",
    "    add = pd.read_csv(file, index_col='date')\n",
    "    print(file, add.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>symbol</th>\n",
       "      <th>Close_Low_avg</th>\n",
       "      <th>Close_High_avg</th>\n",
       "      <th>move_up</th>\n",
       "      <th>wick_high</th>\n",
       "      <th>wick_low</th>\n",
       "      <th>ind__stochK</th>\n",
       "      <th>ind__rsi</th>\n",
       "      <th>ind__mid_bb</th>\n",
       "      <th>ind__up_bb</th>\n",
       "      <th>ind__low_bb</th>\n",
       "      <th>ind__bb_dif</th>\n",
       "      <th>ind__volatile</th>\n",
       "      <th>ind__rsi_accel</th>\n",
       "      <th>ind__rsi_rolling</th>\n",
       "      <th>ind__stoK_accel</th>\n",
       "      <th>ind__stoK_rolling</th>\n",
       "      <th>ind__outside_up</th>\n",
       "      <th>ind__outside_low</th>\n",
       "      <th>ind__surf_pct</th>\n",
       "      <th>spread</th>\n",
       "      <th>bid_diff</th>\n",
       "      <th>ask_diff</th>\n",
       "      <th>ind__full_near_res</th>\n",
       "      <th>ind__full_near_sup</th>\n",
       "      <th>ind__2_3_near_res</th>\n",
       "      <th>ind__2_3_near_sup</th>\n",
       "      <th>ind__1_3_near_res</th>\n",
       "      <th>ind__1_3_near_sup</th>\n",
       "      <th>ind__base_strength</th>\n",
       "      <th>ind__quote_strength</th>\n",
       "      <th>ind__strength_ratio</th>\n",
       "      <th>ind__BS_diff</th>\n",
       "      <th>ind__QS_diff</th>\n",
       "      <th>ind__BS_pastchg</th>\n",
       "      <th>ind__QS_pastchg</th>\n",
       "      <th>ind__midbb_slope</th>\n",
       "      <th>ind__trend_residuals</th>\n",
       "      <th>ind__pub_twit_pol</th>\n",
       "      <th>ind__trad_twit_pol</th>\n",
       "      <th>ind__numtweets_1min</th>\n",
       "      <th>ind__base_mins_to_next</th>\n",
       "      <th>ind__quote_mins_to_next</th>\n",
       "      <th>ind__base_recentevent</th>\n",
       "      <th>ind__quote_recentevent</th>\n",
       "      <th>ind__base_numevents</th>\n",
       "      <th>ind__quote_numevents</th>\n",
       "      <th>ind__base_foreimpact</th>\n",
       "      <th>ind__base_ffalert</th>\n",
       "      <th>ind__base_nextevent_sent</th>\n",
       "      <th>ind__quote_foreimpact</th>\n",
       "      <th>ind__quote_ffalert</th>\n",
       "      <th>ind__quote_nextevent_sent</th>\n",
       "      <th>ind__base_pastimpact</th>\n",
       "      <th>ind__base_pastevent_sent</th>\n",
       "      <th>ind__quote_pastimpact</th>\n",
       "      <th>ind__quote_pastevent_sent</th>\n",
       "      <th>expiration</th>\n",
       "      <th>exch_rate_dif</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:03:00</th>\n",
       "      <td>1.06000</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>1.06009</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059895</td>\n",
       "      <td>1.060095</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>65.000</td>\n",
       "      <td>46.537</td>\n",
       "      <td>1.060094</td>\n",
       "      <td>1.060314</td>\n",
       "      <td>1.059874</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.464</td>\n",
       "      <td>47.2690</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>65.8335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511851</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000e-06</td>\n",
       "      <td>3.040000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:06:00</td>\n",
       "      <td>-0.00013</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:04:00</th>\n",
       "      <td>1.06009</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>1.06003</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059865</td>\n",
       "      <td>1.060065</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>64.167</td>\n",
       "      <td>37.875</td>\n",
       "      <td>1.060096</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>1.059879</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.662</td>\n",
       "      <td>42.2060</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>64.5835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453719</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000e-06</td>\n",
       "      <td>1.870000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:07:00</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:05:00</th>\n",
       "      <td>1.06003</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0596</td>\n",
       "      <td>1.05992</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059760</td>\n",
       "      <td>1.060010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>32.500</td>\n",
       "      <td>26.549</td>\n",
       "      <td>1.060097</td>\n",
       "      <td>1.060311</td>\n",
       "      <td>1.059883</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.326</td>\n",
       "      <td>32.2120</td>\n",
       "      <td>-31.667</td>\n",
       "      <td>48.3335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308428</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000e-06</td>\n",
       "      <td>2.590000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:08:00</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:06:00</th>\n",
       "      <td>1.05992</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0596</td>\n",
       "      <td>1.05996</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059780</td>\n",
       "      <td>1.060030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>11.111</td>\n",
       "      <td>35.338</td>\n",
       "      <td>1.060095</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>1.059877</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0</td>\n",
       "      <td>8.789</td>\n",
       "      <td>30.9435</td>\n",
       "      <td>-21.389</td>\n",
       "      <td>21.8055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208247</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.000000e-07</td>\n",
       "      <td>4.750000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:09:00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:07:00</th>\n",
       "      <td>1.05996</td>\n",
       "      <td>1.0600</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>1.06001</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059855</td>\n",
       "      <td>1.060005</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>37.582</td>\n",
       "      <td>45.523</td>\n",
       "      <td>1.060100</td>\n",
       "      <td>1.060303</td>\n",
       "      <td>1.059897</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0</td>\n",
       "      <td>10.185</td>\n",
       "      <td>40.4305</td>\n",
       "      <td>26.471</td>\n",
       "      <td>24.3465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185047</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000e-06</td>\n",
       "      <td>1.179000e-08</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:10:00</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Open    High     Low    Close  symbol  Close_Low_avg  \\\n",
       "date                                                                           \n",
       "2020-11-16 21:03:00  1.06000  1.0601  1.0597  1.06009  AUDNZD       1.059895   \n",
       "2020-11-16 21:04:00  1.06009  1.0601  1.0597  1.06003  AUDNZD       1.059865   \n",
       "2020-11-16 21:05:00  1.06003  1.0601  1.0596  1.05992  AUDNZD       1.059760   \n",
       "2020-11-16 21:06:00  1.05992  1.0601  1.0596  1.05996  AUDNZD       1.059780   \n",
       "2020-11-16 21:07:00  1.05996  1.0600  1.0597  1.06001  AUDNZD       1.059855   \n",
       "\n",
       "                     Close_High_avg  move_up  wick_high  wick_low  \\\n",
       "date                                                                \n",
       "2020-11-16 21:03:00        1.060095        1    0.00001   0.00030   \n",
       "2020-11-16 21:04:00        1.060065        0    0.00001   0.00033   \n",
       "2020-11-16 21:05:00        1.060010        0    0.00007   0.00032   \n",
       "2020-11-16 21:06:00        1.060030        1    0.00014   0.00032   \n",
       "2020-11-16 21:07:00        1.060005        1   -0.00001   0.00026   \n",
       "\n",
       "                     ind__stochK  ind__rsi  ind__mid_bb  ind__up_bb  \\\n",
       "date                                                                  \n",
       "2020-11-16 21:03:00       65.000    46.537     1.060094    1.060314   \n",
       "2020-11-16 21:04:00       64.167    37.875     1.060096    1.060313   \n",
       "2020-11-16 21:05:00       32.500    26.549     1.060097    1.060311   \n",
       "2020-11-16 21:06:00       11.111    35.338     1.060095    1.060313   \n",
       "2020-11-16 21:07:00       37.582    45.523     1.060100    1.060303   \n",
       "\n",
       "                     ind__low_bb  ind__bb_dif  ind__volatile  ind__rsi_accel  \\\n",
       "date                                                                           \n",
       "2020-11-16 21:03:00     1.059874     0.000440              0          -1.464   \n",
       "2020-11-16 21:04:00     1.059879     0.000434              0          -8.662   \n",
       "2020-11-16 21:05:00     1.059883     0.000428              0         -11.326   \n",
       "2020-11-16 21:06:00     1.059877     0.000436              0           8.789   \n",
       "2020-11-16 21:07:00     1.059897     0.000406              0          10.185   \n",
       "\n",
       "                     ind__rsi_rolling  ind__stoK_accel  ind__stoK_rolling  \\\n",
       "date                                                                        \n",
       "2020-11-16 21:03:00           47.2690           -1.667            65.8335   \n",
       "2020-11-16 21:04:00           42.2060           -0.833            64.5835   \n",
       "2020-11-16 21:05:00           32.2120          -31.667            48.3335   \n",
       "2020-11-16 21:06:00           30.9435          -21.389            21.8055   \n",
       "2020-11-16 21:07:00           40.4305           26.471            24.3465   \n",
       "\n",
       "                     ind__outside_up  ind__outside_low  ind__surf_pct  \\\n",
       "date                                                                    \n",
       "2020-11-16 21:03:00                0                 0       0.511851   \n",
       "2020-11-16 21:04:00                0                 0       0.453719   \n",
       "2020-11-16 21:05:00                0                 0       0.308428   \n",
       "2020-11-16 21:06:00                0                 0       0.208247   \n",
       "2020-11-16 21:07:00                0                 0       0.185047   \n",
       "\n",
       "                       spread  bid_diff  ask_diff  ind__full_near_res  \\\n",
       "date                                                                    \n",
       "2020-11-16 21:03:00  0.000080  0.000048  0.000032                   0   \n",
       "2020-11-16 21:04:00  0.000041  0.000023  0.000018                   0   \n",
       "2020-11-16 21:05:00  0.000060  0.000030  0.000030                   0   \n",
       "2020-11-16 21:06:00  0.000061  0.000041  0.000020                   0   \n",
       "2020-11-16 21:07:00  0.000089  0.000044  0.000044                   0   \n",
       "\n",
       "                     ind__full_near_sup  ind__2_3_near_res  ind__2_3_near_sup  \\\n",
       "date                                                                            \n",
       "2020-11-16 21:03:00                   0                  0                  1   \n",
       "2020-11-16 21:04:00                   0                  0                  0   \n",
       "2020-11-16 21:05:00                   0                  0                  0   \n",
       "2020-11-16 21:06:00                   0                  0                  0   \n",
       "2020-11-16 21:07:00                   0                  0                  1   \n",
       "\n",
       "                     ind__1_3_near_res  ind__1_3_near_sup  ind__base_strength  \\\n",
       "date                                                                            \n",
       "2020-11-16 21:03:00                  0                  0                   6   \n",
       "2020-11-16 21:04:00                  0                  0                   6   \n",
       "2020-11-16 21:05:00                  0                  0                   6   \n",
       "2020-11-16 21:06:00                  0                  1                   6   \n",
       "2020-11-16 21:07:00                  0                  1                   6   \n",
       "\n",
       "                     ind__quote_strength  ind__strength_ratio  ind__BS_diff  \\\n",
       "date                                                                          \n",
       "2020-11-16 21:03:00                    6                  1.0             0   \n",
       "2020-11-16 21:04:00                    6                  1.0             0   \n",
       "2020-11-16 21:05:00                    6                  1.0             0   \n",
       "2020-11-16 21:06:00                    6                  1.0             0   \n",
       "2020-11-16 21:07:00                    6                  1.0             0   \n",
       "\n",
       "                     ind__QS_diff  ind__BS_pastchg  ind__QS_pastchg  \\\n",
       "date                                                                  \n",
       "2020-11-16 21:03:00             0                0                0   \n",
       "2020-11-16 21:04:00             0                0                0   \n",
       "2020-11-16 21:05:00             0                0                0   \n",
       "2020-11-16 21:06:00             0                0                0   \n",
       "2020-11-16 21:07:00             0                0                0   \n",
       "\n",
       "                     ind__midbb_slope  ind__trend_residuals  \\\n",
       "date                                                          \n",
       "2020-11-16 21:03:00      7.000000e-06          3.040000e-09   \n",
       "2020-11-16 21:04:00      3.000000e-06          1.870000e-09   \n",
       "2020-11-16 21:05:00      1.500000e-06          2.590000e-09   \n",
       "2020-11-16 21:06:00     -5.000000e-07          4.750000e-09   \n",
       "2020-11-16 21:07:00      1.500000e-06          1.179000e-08   \n",
       "\n",
       "                     ind__pub_twit_pol  ind__trad_twit_pol  \\\n",
       "date                                                         \n",
       "2020-11-16 21:03:00          -0.142857                 1.0   \n",
       "2020-11-16 21:04:00          -0.142857                 1.0   \n",
       "2020-11-16 21:05:00          -0.142857                 1.0   \n",
       "2020-11-16 21:06:00          -0.142857                 1.0   \n",
       "2020-11-16 21:07:00          -0.142857                 1.0   \n",
       "\n",
       "                     ind__numtweets_1min ind__base_mins_to_next  \\\n",
       "date                                                              \n",
       "2020-11-16 21:03:00                    0                     56   \n",
       "2020-11-16 21:04:00                    0                     55   \n",
       "2020-11-16 21:05:00                    0                     54   \n",
       "2020-11-16 21:06:00                    0                     53   \n",
       "2020-11-16 21:07:00                    0                     52   \n",
       "\n",
       "                    ind__quote_mins_to_next  ind__base_recentevent  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00                    41.0                      0   \n",
       "2020-11-16 21:04:00                    40.0                      0   \n",
       "2020-11-16 21:05:00                    39.0                      0   \n",
       "2020-11-16 21:06:00                    38.0                      0   \n",
       "2020-11-16 21:07:00                    37.0                      0   \n",
       "\n",
       "                     ind__quote_recentevent  ind__base_numevents  \\\n",
       "date                                                               \n",
       "2020-11-16 21:03:00                       0                    5   \n",
       "2020-11-16 21:04:00                       0                    5   \n",
       "2020-11-16 21:05:00                       0                    5   \n",
       "2020-11-16 21:06:00                       0                    5   \n",
       "2020-11-16 21:07:00                       0                    5   \n",
       "\n",
       "                     ind__quote_numevents ind__base_foreimpact  \\\n",
       "date                                                             \n",
       "2020-11-16 21:03:00                     2            high_none   \n",
       "2020-11-16 21:04:00                     2            high_none   \n",
       "2020-11-16 21:05:00                     2            high_none   \n",
       "2020-11-16 21:06:00                     2            high_none   \n",
       "2020-11-16 21:07:00                     2            high_none   \n",
       "\n",
       "                    ind__base_ffalert ind__base_nextevent_sent  \\\n",
       "date                                                             \n",
       "2020-11-16 21:03:00              none            high_negative   \n",
       "2020-11-16 21:04:00              none            high_negative   \n",
       "2020-11-16 21:05:00              none            high_negative   \n",
       "2020-11-16 21:06:00              none            high_negative   \n",
       "2020-11-16 21:07:00              none            high_negative   \n",
       "\n",
       "                    ind__quote_foreimpact ind__quote_ffalert  \\\n",
       "date                                                           \n",
       "2020-11-16 21:03:00    multiple_low_right               none   \n",
       "2020-11-16 21:04:00    multiple_low_right               none   \n",
       "2020-11-16 21:05:00    multiple_low_right               none   \n",
       "2020-11-16 21:06:00    multiple_low_right               none   \n",
       "2020-11-16 21:07:00    multiple_low_right               none   \n",
       "\n",
       "                    ind__quote_nextevent_sent ind__base_pastimpact  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:04:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:05:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:06:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:07:00      multiple_low_neutral                 none   \n",
       "\n",
       "                    ind__base_pastevent_sent ind__quote_pastimpact  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:04:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:05:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:06:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:07:00     multiple_low_neutral                  none   \n",
       "\n",
       "                    ind__quote_pastevent_sent           expiration  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00                      none  2020-11-16 21:06:00   \n",
       "2020-11-16 21:04:00                      none  2020-11-16 21:07:00   \n",
       "2020-11-16 21:05:00                      none  2020-11-16 21:08:00   \n",
       "2020-11-16 21:06:00                      none  2020-11-16 21:09:00   \n",
       "2020-11-16 21:07:00                      none  2020-11-16 21:10:00   \n",
       "\n",
       "                     exch_rate_dif direction  \n",
       "date                                          \n",
       "2020-11-16 21:03:00       -0.00013      down  \n",
       "2020-11-16 21:04:00       -0.00002      down  \n",
       "2020-11-16 21:05:00        0.00024        up  \n",
       "2020-11-16 21:06:00        0.00000       NaN  \n",
       "2020-11-16 21:07:00       -0.00002      down  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = {}\n",
    "df = pd.DataFrame()\n",
    "for i,file in enumerate(glob.glob(data_path)):\n",
    "    add = pd.read_csv(file, index_col='date')\n",
    "    dfs['df_'+str(i)] = add\n",
    "    df = pd.concat([df, add])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41797, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close_Low_avg</th>\n",
       "      <th>Close_High_avg</th>\n",
       "      <th>move_up</th>\n",
       "      <th>wick_high</th>\n",
       "      <th>wick_low</th>\n",
       "      <th>ind__stochK</th>\n",
       "      <th>ind__rsi</th>\n",
       "      <th>ind__mid_bb</th>\n",
       "      <th>ind__up_bb</th>\n",
       "      <th>ind__low_bb</th>\n",
       "      <th>ind__bb_dif</th>\n",
       "      <th>ind__volatile</th>\n",
       "      <th>ind__rsi_accel</th>\n",
       "      <th>ind__rsi_rolling</th>\n",
       "      <th>ind__stoK_accel</th>\n",
       "      <th>ind__stoK_rolling</th>\n",
       "      <th>ind__outside_up</th>\n",
       "      <th>ind__outside_low</th>\n",
       "      <th>ind__surf_pct</th>\n",
       "      <th>spread</th>\n",
       "      <th>bid_diff</th>\n",
       "      <th>ask_diff</th>\n",
       "      <th>ind__full_near_res</th>\n",
       "      <th>ind__full_near_sup</th>\n",
       "      <th>ind__2_3_near_res</th>\n",
       "      <th>ind__2_3_near_sup</th>\n",
       "      <th>ind__1_3_near_res</th>\n",
       "      <th>ind__1_3_near_sup</th>\n",
       "      <th>ind__base_strength</th>\n",
       "      <th>ind__quote_strength</th>\n",
       "      <th>ind__strength_ratio</th>\n",
       "      <th>ind__BS_diff</th>\n",
       "      <th>ind__QS_diff</th>\n",
       "      <th>ind__BS_pastchg</th>\n",
       "      <th>ind__QS_pastchg</th>\n",
       "      <th>ind__midbb_slope</th>\n",
       "      <th>ind__trend_residuals</th>\n",
       "      <th>ind__pub_twit_pol</th>\n",
       "      <th>ind__trad_twit_pol</th>\n",
       "      <th>ind__numtweets_1min</th>\n",
       "      <th>ind__base_recentevent</th>\n",
       "      <th>ind__quote_recentevent</th>\n",
       "      <th>ind__base_numevents</th>\n",
       "      <th>ind__quote_numevents</th>\n",
       "      <th>exch_rate_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.00000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>4.179700e+04</td>\n",
       "      <td>4.179700e+04</td>\n",
       "      <td>36493.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.126048</td>\n",
       "      <td>35.129885</td>\n",
       "      <td>35.116332</td>\n",
       "      <td>35.126100</td>\n",
       "      <td>35.121216</td>\n",
       "      <td>35.127992</td>\n",
       "      <td>0.479269</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>51.277724</td>\n",
       "      <td>49.959910</td>\n",
       "      <td>35.125582</td>\n",
       "      <td>35.136788</td>\n",
       "      <td>35.114376</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.245592</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>49.958861</td>\n",
       "      <td>0.015565</td>\n",
       "      <td>51.269941</td>\n",
       "      <td>0.039955</td>\n",
       "      <td>0.047850</td>\n",
       "      <td>0.498260</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.239778</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.223485</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.204704</td>\n",
       "      <td>0.196306</td>\n",
       "      <td>3.149819</td>\n",
       "      <td>3.018470</td>\n",
       "      <td>1.458237</td>\n",
       "      <td>-0.000550</td>\n",
       "      <td>-0.00012</td>\n",
       "      <td>0.025863</td>\n",
       "      <td>0.029213</td>\n",
       "      <td>5.654986e-05</td>\n",
       "      <td>8.866025e-05</td>\n",
       "      <td>0.093566</td>\n",
       "      <td>-0.204974</td>\n",
       "      <td>0.381606</td>\n",
       "      <td>0.096083</td>\n",
       "      <td>0.050889</td>\n",
       "      <td>4.451994</td>\n",
       "      <td>2.120990</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>53.215940</td>\n",
       "      <td>53.221795</td>\n",
       "      <td>53.201167</td>\n",
       "      <td>53.216032</td>\n",
       "      <td>53.208599</td>\n",
       "      <td>53.218913</td>\n",
       "      <td>0.499576</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.013856</td>\n",
       "      <td>33.691517</td>\n",
       "      <td>16.204582</td>\n",
       "      <td>53.215120</td>\n",
       "      <td>53.232096</td>\n",
       "      <td>53.198147</td>\n",
       "      <td>0.041234</td>\n",
       "      <td>0.430443</td>\n",
       "      <td>12.816258</td>\n",
       "      <td>14.884937</td>\n",
       "      <td>25.068982</td>\n",
       "      <td>31.269449</td>\n",
       "      <td>0.195856</td>\n",
       "      <td>0.213452</td>\n",
       "      <td>0.277337</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>0.406418</td>\n",
       "      <td>0.416586</td>\n",
       "      <td>0.412815</td>\n",
       "      <td>0.403490</td>\n",
       "      <td>0.397207</td>\n",
       "      <td>1.591439</td>\n",
       "      <td>1.729962</td>\n",
       "      <td>1.441735</td>\n",
       "      <td>0.469089</td>\n",
       "      <td>0.48843</td>\n",
       "      <td>0.158729</td>\n",
       "      <td>0.168404</td>\n",
       "      <td>1.316100e-03</td>\n",
       "      <td>2.906878e-04</td>\n",
       "      <td>0.293084</td>\n",
       "      <td>0.530156</td>\n",
       "      <td>2.478906</td>\n",
       "      <td>0.294709</td>\n",
       "      <td>0.219773</td>\n",
       "      <td>4.849597</td>\n",
       "      <td>2.551035</td>\n",
       "      <td>0.011670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.727260</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>0.727260</td>\n",
       "      <td>0.727115</td>\n",
       "      <td>0.727425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.021000</td>\n",
       "      <td>-0.029000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.534000</td>\n",
       "      <td>0.727666</td>\n",
       "      <td>0.727901</td>\n",
       "      <td>0.727015</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.862000</td>\n",
       "      <td>1.833500</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.328497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-6.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.227500e-02</td>\n",
       "      <td>6.675654e-32</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.301500</td>\n",
       "      <td>1.301700</td>\n",
       "      <td>1.301100</td>\n",
       "      <td>1.301500</td>\n",
       "      <td>1.301300</td>\n",
       "      <td>1.301600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>21.429000</td>\n",
       "      <td>38.970000</td>\n",
       "      <td>1.301530</td>\n",
       "      <td>1.302062</td>\n",
       "      <td>1.300969</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.355000</td>\n",
       "      <td>40.308500</td>\n",
       "      <td>-15.574000</td>\n",
       "      <td>24.576000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275184</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.750000e-05</td>\n",
       "      <td>1.996000e-08</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359600</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.377000</td>\n",
       "      <td>1.359352</td>\n",
       "      <td>1.360176</td>\n",
       "      <td>1.358642</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.118000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.499500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499580</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.979041e-16</td>\n",
       "      <td>8.428000e-08</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.399000</td>\n",
       "      <td>104.407000</td>\n",
       "      <td>104.370000</td>\n",
       "      <td>104.399000</td>\n",
       "      <td>104.384500</td>\n",
       "      <td>104.403000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>61.027000</td>\n",
       "      <td>104.400350</td>\n",
       "      <td>104.430120</td>\n",
       "      <td>104.365925</td>\n",
       "      <td>0.036704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.283000</td>\n",
       "      <td>59.696500</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>78.316000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722353</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000e-05</td>\n",
       "      <td>3.670000e-05</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>126.672000</td>\n",
       "      <td>126.682000</td>\n",
       "      <td>126.630000</td>\n",
       "      <td>126.672000</td>\n",
       "      <td>126.644500</td>\n",
       "      <td>126.672000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.383000</td>\n",
       "      <td>126.631800</td>\n",
       "      <td>126.716502</td>\n",
       "      <td>126.608901</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.724000</td>\n",
       "      <td>96.789500</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.284345</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.785000e-02</td>\n",
       "      <td>1.046520e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close  Close_Low_avg  \\\n",
       "count  41797.000000  41797.000000  41797.000000  41797.000000   41797.000000   \n",
       "mean      35.126048     35.129885     35.116332     35.126100      35.121216   \n",
       "std       53.215940     53.221795     53.201167     53.216032      53.208599   \n",
       "min        0.727260      0.727500      0.726800      0.727260       0.727115   \n",
       "25%        1.301500      1.301700      1.301100      1.301500       1.301300   \n",
       "50%        1.359400      1.359600      1.359400      1.359400       1.359400   \n",
       "75%      104.399000    104.407000    104.370000    104.399000     104.384500   \n",
       "max      126.672000    126.682000    126.630000    126.672000     126.644500   \n",
       "\n",
       "       Close_High_avg       move_up     wick_high      wick_low   ind__stochK  \\\n",
       "count    41797.000000  41797.000000  41797.000000  41797.000000  41797.000000   \n",
       "mean        35.127992      0.479269      0.002248      0.008179     51.277724   \n",
       "std         53.218913      0.499576      0.004989      0.013856     33.691517   \n",
       "min          0.727425      0.000000     -0.021000     -0.029000      0.000000   \n",
       "25%          1.301600      0.000000      0.000030      0.000200     21.429000   \n",
       "50%          1.359515      0.000000      0.000100      0.000380     50.000000   \n",
       "75%        104.403000      1.000000      0.002000      0.017000     82.500000   \n",
       "max        126.672000      1.000000      0.075000      0.112000    100.000000   \n",
       "\n",
       "           ind__rsi   ind__mid_bb    ind__up_bb   ind__low_bb   ind__bb_dif  \\\n",
       "count  41797.000000  41797.000000  41797.000000  41797.000000  41797.000000   \n",
       "mean      49.959910     35.125582     35.136788     35.114376      0.022411   \n",
       "std       16.204582     53.215120     53.232096     53.198147      0.041234   \n",
       "min        1.534000      0.727666      0.727901      0.727015      0.000104   \n",
       "25%       38.970000      1.301530      1.302062      1.300969      0.000734   \n",
       "50%       50.377000      1.359352      1.360176      1.358642      0.001224   \n",
       "75%       61.027000    104.400350    104.430120    104.365925      0.036704   \n",
       "max       97.383000    126.631800    126.716502    126.608901      0.461340   \n",
       "\n",
       "       ind__volatile  ind__rsi_accel  ind__rsi_rolling  ind__stoK_accel  \\\n",
       "count   41797.000000    41797.000000      41797.000000     41797.000000   \n",
       "mean        0.245592        0.002098         49.958861         0.015565   \n",
       "std         0.430443       12.816258         14.884937        25.068982   \n",
       "min         0.000000      -56.862000          1.833500       -50.000000   \n",
       "25%         0.000000       -8.355000         40.308500       -15.574000   \n",
       "50%         0.000000        0.000000         50.118000         0.000000   \n",
       "75%         0.000000        8.283000         59.696500        15.625000   \n",
       "max         1.000000       56.724000         96.789500        50.000000   \n",
       "\n",
       "       ind__stoK_rolling  ind__outside_up  ind__outside_low  ind__surf_pct  \\\n",
       "count       41797.000000     41797.000000      41797.000000   41797.000000   \n",
       "mean           51.269941         0.039955          0.047850       0.498260   \n",
       "std            31.269449         0.195856          0.213452       0.277337   \n",
       "min             0.000000         0.000000          0.000000      -0.328497   \n",
       "25%            24.576000         0.000000          0.000000       0.275184   \n",
       "50%            51.499500         0.000000          0.000000       0.499580   \n",
       "75%            78.316000         0.000000          0.000000       0.722353   \n",
       "max           100.000000         1.000000          1.000000       1.284345   \n",
       "\n",
       "             spread      bid_diff      ask_diff  ind__full_near_res  \\\n",
       "count  41797.000000  41797.000000  41797.000000        41797.000000   \n",
       "mean       0.001756      0.000875      0.000881            0.239778   \n",
       "std        0.002972      0.001613      0.001622            0.426953   \n",
       "min        0.000000      0.000000      0.000000            0.000000   \n",
       "25%        0.000051      0.000022      0.000022            0.000000   \n",
       "50%        0.000082      0.000045      0.000045            0.000000   \n",
       "75%        0.003006      0.000803      0.000814            0.000000   \n",
       "max        0.012637      0.006319      0.006319            1.000000   \n",
       "\n",
       "       ind__full_near_sup  ind__2_3_near_res  ind__2_3_near_sup  \\\n",
       "count        41797.000000       41797.000000       41797.000000   \n",
       "mean             0.208747           0.223485           0.217886   \n",
       "std              0.406418           0.416586           0.412815   \n",
       "min              0.000000           0.000000           0.000000   \n",
       "25%              0.000000           0.000000           0.000000   \n",
       "50%              0.000000           0.000000           0.000000   \n",
       "75%              0.000000           0.000000           0.000000   \n",
       "max              1.000000           1.000000           1.000000   \n",
       "\n",
       "       ind__1_3_near_res  ind__1_3_near_sup  ind__base_strength  \\\n",
       "count       41797.000000       41797.000000        41797.000000   \n",
       "mean            0.204704           0.196306            3.149819   \n",
       "std             0.403490           0.397207            1.591439   \n",
       "min             0.000000           0.000000            0.000000   \n",
       "25%             0.000000           0.000000            2.000000   \n",
       "50%             0.000000           0.000000            3.000000   \n",
       "75%             0.000000           0.000000            4.000000   \n",
       "max             1.000000           1.000000            6.000000   \n",
       "\n",
       "       ind__quote_strength  ind__strength_ratio  ind__BS_diff  ind__QS_diff  \\\n",
       "count         41797.000000         41797.000000  41797.000000   41797.00000   \n",
       "mean              3.018470             1.458237     -0.000550      -0.00012   \n",
       "std               1.729962             1.441735      0.469089       0.48843   \n",
       "min               0.000000             0.000000     -6.000000      -6.00000   \n",
       "25%               1.000000             0.400000      0.000000       0.00000   \n",
       "50%               3.000000             1.000000      0.000000       0.00000   \n",
       "75%               4.000000             2.000000      0.000000       0.00000   \n",
       "max               6.000000             6.000000      6.000000       6.00000   \n",
       "\n",
       "       ind__BS_pastchg  ind__QS_pastchg  ind__midbb_slope  \\\n",
       "count     41797.000000     41797.000000      4.179700e+04   \n",
       "mean          0.025863         0.029213      5.654986e-05   \n",
       "std           0.158729         0.168404      1.316100e-03   \n",
       "min           0.000000         0.000000     -1.227500e-02   \n",
       "25%           0.000000         0.000000     -2.750000e-05   \n",
       "50%           0.000000         0.000000     -2.979041e-16   \n",
       "75%           0.000000         0.000000      2.900000e-05   \n",
       "max           1.000000         1.000000      1.785000e-02   \n",
       "\n",
       "       ind__trend_residuals  ind__pub_twit_pol  ind__trad_twit_pol  \\\n",
       "count          4.179700e+04       36493.000000        41797.000000   \n",
       "mean           8.866025e-05           0.093566           -0.204974   \n",
       "std            2.906878e-04           0.293084            0.530156   \n",
       "min            6.675654e-32          -1.000000           -1.000000   \n",
       "25%            1.996000e-08          -0.033333           -0.647059   \n",
       "50%            8.428000e-08           0.066667            0.000000   \n",
       "75%            3.670000e-05           0.228571            0.000000   \n",
       "max            1.046520e-02           1.000000            1.000000   \n",
       "\n",
       "       ind__numtweets_1min  ind__base_recentevent  ind__quote_recentevent  \\\n",
       "count         41797.000000           41797.000000            41797.000000   \n",
       "mean              0.381606               0.096083                0.050889   \n",
       "std               2.478906               0.294709                0.219773   \n",
       "min               0.000000               0.000000                0.000000   \n",
       "25%               0.000000               0.000000                0.000000   \n",
       "50%               0.000000               0.000000                0.000000   \n",
       "75%               0.000000               0.000000                0.000000   \n",
       "max             192.000000               1.000000                1.000000   \n",
       "\n",
       "       ind__base_numevents  ind__quote_numevents  exch_rate_dif  \n",
       "count         41797.000000          41797.000000   41797.000000  \n",
       "mean              4.451994              2.120990       0.000168  \n",
       "std               4.849597              2.551035       0.011670  \n",
       "min               0.000000              0.000000      -0.176000  \n",
       "25%               0.000000              0.000000      -0.000270  \n",
       "50%               3.000000              1.000000       0.000000  \n",
       "75%               6.000000              3.000000       0.000290  \n",
       "max              20.000000             14.000000       0.143000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows in the dataframe above were created using the \"forex_datamining\" notebook. The functions below will prepare the columns for model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bold(_str):\n",
    "    print(\"\\033[1m\" + _str + \"\\033[0m\")\n",
    "    \n",
    "\n",
    "def get_event_info(base_event, quote_event, level=None, outcome=None):\n",
    "    '''\n",
    "    - level - set to 'high','medium', or 'low'\n",
    "    - outcome - if None, then returning count of events or number of events with FF Alerts\n",
    "        - if assigned value, then returning the result actual, forecast, or sentiment\n",
    "            - this will be based on the input 'base_event','quote_event' arguments\n",
    "    '''\n",
    "    results = {'high':[],'medium':[],'low':[]}\n",
    "    for event in [base_event, quote_event]:\n",
    "        if event == 'none':\n",
    "            # all results stay empty\n",
    "            pass\n",
    "        elif 'multiple' in event:\n",
    "            # what ever impact and result is given, return 2 times that\n",
    "            items = event.split('_')\n",
    "            for key in results.keys():\n",
    "                if items[1]==key:\n",
    "                    # append this value twice for having 'multiple' same event types and results\n",
    "                    results[key].append(items[2])\n",
    "                    results[key].append(items[2])\n",
    "        else:\n",
    "            items = event.split('__')\n",
    "            for item in items:\n",
    "                it = item.split('_')\n",
    "                for key in results.keys():\n",
    "                    if it[0]==key: results[key].append(it[1])\n",
    "    # check what we are wanting to return \n",
    "    if outcome is None:\n",
    "        return len(results[level])\n",
    "    \n",
    "    # if 'outcome' is assigned a value - identify actual, forecast or sentiment result\n",
    "    if base_event==quote_event=='none': out = 0\n",
    "    else:\n",
    "        res_dict = {}\n",
    "        for key in results.keys():\n",
    "            actual = []\n",
    "            if len(results[key])==0: pass\n",
    "            else:\n",
    "                for res in results[key]:\n",
    "                    # 'better' - actual, 'right' - forecast, 'positive' - sentiment\n",
    "                    if res in ['better','right','positive']: actual.append(1)\n",
    "                    elif res in ['worse','wrong','negative']: actual.append(-1)\n",
    "            res_dict[key] = actual\n",
    "        actual_res = 3*sum(res_dict['high']) + 2*sum(res_dict['medium']) + sum(res_dict['low'])\n",
    "        if actual_res>0 and outcome=='positive': out = 1\n",
    "        elif actual_res<0 and outcome=='negative': out = 1\n",
    "        elif actual_res==0 and outcome=='neutral': out = 1\n",
    "        else: out = 0\n",
    "    return out\n",
    "\n",
    "\n",
    "def feature_engineering(df, filter_out=True, corr_co=0.90):\n",
    "    \n",
    "    # drop any null values\n",
    "    df = df.copy().dropna()\n",
    "    \n",
    "    # create a list of features to drop after feature engineering\n",
    "    to_drop = []\n",
    "    for feature in df.columns:\n",
    "        if df[feature].dtype == 'O' and ('__base_' in feature or '__quote_' in feature):\n",
    "            to_drop.append(feature)\n",
    "    \n",
    "    # this will include all feature engineering to the above OBJECT categorical features\n",
    "    # assuming NA values are taken care of\n",
    "    df['ind__base_mins2next'] = df.ind__base_mins_to_next.apply(lambda x: 2088 if x=='none' else float(x))\n",
    "    df['ind__quote_mins2next'] = df.ind__quote_mins_to_next.apply(lambda x: 2088 if x=='none' else float(x))\n",
    "    \n",
    "    # create binary indicator for whether or not a base/quote currency has the next event coming up (not including 2088 values)\n",
    "    df['ind__base_nextevent'] = df.apply(lambda x: 1 if (x['ind__base_mins2next'] >= x['ind__quote_mins2next'] and\\\n",
    "                                                     x['ind__base_mins2next']!=2088) else 0, axis=1)\n",
    "    df['ind__quote_nextevent'] = df.apply(lambda x: 1 if (x['ind__quote_mins2next'] >= x['ind__base_mins2next'] and\\\n",
    "                                                     x['ind__quote_mins2next']!=2088) else 0, axis=1)\n",
    "    # past event impacts    \n",
    "    df['ind__pastimp_high'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                             level='high'), axis=1)\n",
    "    df['ind__pastimp_med'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                             level='medium'), axis=1)\n",
    "    df['ind__pastimp_low'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                             level='low'), axis=1)\n",
    "    df['ind__past_actual_pos'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__past_actual_neg'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__past_actual_neut'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    # forecast of event impacts    \n",
    "    df['ind__nextimp_high'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                             level='high'), axis=1)\n",
    "    df['ind__nextimp_med'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                             level='medium'), axis=1)\n",
    "    df['ind__nextimp_low'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                             level='low'), axis=1)\n",
    "    df['ind__next_fore_pos'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__next_fore_neg'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__next_fore_neut'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    # set FF Alert features for next events\n",
    "    df['ind__ffalert_high']=df.apply(lambda x:get_event_info(x['ind__base_ffalert'],x['ind__quote_ffalert'],level='high'), axis=1)\n",
    "    df['ind__ffalert_med']=df.apply(lambda x:get_event_info(x['ind__base_ffalert'],x['ind__quote_ffalert'],level='medium'), axis=1)\n",
    "    df['ind__ffalert_low']=df.apply(lambda x:get_event_info(x['ind__base_ffalert'],x['ind__quote_ffalert'],level='low'), axis=1)\n",
    "    # set sentiment analysis features\n",
    "    df['ind__pastimp_sent_pos'] = df.apply(lambda x:get_event_info(x['ind__base_pastevent_sent'],x['ind__quote_pastevent_sent'],\\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__pastimp_sent_neg'] = df.apply(lambda x:get_event_info(x['ind__base_pastevent_sent'],x['ind__quote_pastevent_sent'],\\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__pastimp_sent_neut'] = df.apply(lambda x:get_event_info(x['ind__base_pastevent_sent'],x['ind__quote_pastevent_sent'],\\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    df['ind__nextimp_sent_pos'] = df.apply(lambda x:get_event_info(x['ind__base_nextevent_sent'],x['ind__quote_nextevent_sent'],\\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__nextimp_sent_neg'] = df.apply(lambda x:get_event_info(x['ind__base_nextevent_sent'],x['ind__quote_nextevent_sent'],\\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__nextimp_sent_neut'] = df.apply(lambda x:get_event_info(x['ind__base_nextevent_sent'],x['ind__quote_nextevent_sent'],\\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    # now we should drop all features in 'to_drop' that were used to create these features\n",
    "    df = df.drop(to_drop, axis=1)\n",
    "    \n",
    "    # we are going to make Low == to lowest price if not already\n",
    "    # same for High\n",
    "    df.Low = df.apply(lambda x: x['Low'] if x['Low'] == min([x['Low'],x['High'],x['Open'],x['Close']]) else \\\n",
    "                         min([x['Low'],x['High'],x['Open'],x['Close']]), axis=1)\n",
    "    df.High = df.apply(lambda x: x['High'] if x['High'] == max([x['Low'],x['High'],x['Open'],x['Close']]) else \\\n",
    "                         max([x['Low'],x['High'],x['Open'],x['Close']]), axis=1)\n",
    "    \n",
    "    # set negative values to 0 where wick height was taken\n",
    "    df.wick_high = df.wick_high.apply(lambda x: 0 if x<0 else x)\n",
    "    df.wick_low = df.wick_low.apply(lambda x: 0 if x<0 else x)\n",
    "    \n",
    "    # convert features to scale over their exchange rates to normalize the rows\n",
    "    # best way to deal with wicks -- we will try wick / range pct\n",
    "    df['wick_low_pct'] = df.wick_low / (df.High - df.Low)\n",
    "    df['wick_high_pct'] = df.wick_high / (df.High - df.Low)\n",
    "    df['ind__midbb_pct'] = 10000*(df.Open - df.ind__mid_bb) / df.Open\n",
    "    df['ind__bbdif_pct'] = 10000*df.ind__bb_dif / df.ind__low_bb \n",
    "    df['spread_pct'] = 10000*df.spread/df.Close\n",
    "    #df['biddif_pct'] = df.bid_diff/df.spread #  -- only need one, the other is infered -- otherwise singularity potential\n",
    "    df['askdif_pct'] = df.ask_diff/df.spread\n",
    "    df['ind__bbslope_pct'] = 10000*df.ind__midbb_slope / df.Close\n",
    "    # np.polyfit calculates the residuals as sum of squared errors -- therefore divide by Close**2 to normalize\n",
    "    df['ind__trendresids_pct'] = 10000*df.ind__trend_residuals / df.Close**2\n",
    "    \n",
    "    # define the target variable\n",
    "    df = df[df.exch_rate_dif!=0]\n",
    "    df['exratedif_pct'] = 10000*df.exch_rate_dif/df.Close\n",
    "    df['dir__up_1__down_0'] = df.direction.apply(lambda val: 1 if val=='up' else 0)\n",
    "    df.drop(['exch_rate_dif','direction'],axis=1,inplace=True)\n",
    "    \n",
    "    # drop remaining features that seperate symbols by rate\n",
    "    drop_other = ['Close_Low_avg','Close_High_avg','ind__up_bb','ind__low_bb','expiration','Close','Open','Low','High',\n",
    "                  'wick_high','wick_low','ind__bb_dif','spread','bid_diff','ask_diff','ind__midbb_slope','ind__mid_bb',\n",
    "                  'ind__trend_residuals']\n",
    "    df = df.drop(drop_other, axis=1)\n",
    "    \n",
    "    # we will create dummy variable for the symbol features, and add on remaining symbols\n",
    "    ## WE ARE GOING TO SUPPRESS THESE VALUES, MODEL PERFORMS BETTER WITHOUT THEM\n",
    "#     symbol_dummies = pd.get_dummies(df.symbol, drop_first=True)\n",
    "#     df = pd.concat([df,symbol_dummies], axis=1)\n",
    "#     df = df.drop(['symbol'], axis=1)\n",
    "\n",
    "    # filter out where we did not pull currency strengths\n",
    "    df = df[df.ind__base_strength!=0]\n",
    "    \n",
    "    # filter out features that are a constant value (where only 1 or 0 was recorded in data)\n",
    "    to_drop=[]\n",
    "    for col in df.describe().columns:\n",
    "        desc = df.describe()[col]\n",
    "        if desc['mean']==0 and desc['max']-desc['min']==0:\n",
    "            to_drop.append(col)\n",
    "    if len(to_drop)>0:\n",
    "        print_bold('DROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:')\n",
    "        print(to_drop)\n",
    "        df = df.drop(to_drop, axis=1)\n",
    "        \n",
    "    # create a list of features based on correlation priority\n",
    "    corr = df.drop(['exratedif_pct'],axis=1).corr()['dir__up_1__down_0']\n",
    "    corr_priority = abs(corr).sort_values(ascending=False)[1:]\n",
    "    # filter out features that are too correlated\n",
    "    corr_drop=[]\n",
    "    if filter_out:\n",
    "        corrs = df.drop(['exratedif_pct','dir__up_1__down_0'],axis=1).corr()\n",
    "        for row in corrs.index:\n",
    "            for col in corrs.columns:\n",
    "                if row == col: break\n",
    "                else:\n",
    "                    if abs(corrs.loc[row,col]) > corr_co:\n",
    "                        # check which feature has a higher correlation to the target feature and drop the smaller\n",
    "                        if corr_priority[col] >= corr_priority[row]: corr_drop.append(row)\n",
    "                        else: corr_drop.append(col)\n",
    "        corr_drop = np.unique(corr_drop).tolist()\n",
    "        if len(corr_drop)>0:\n",
    "            print_bold('\\nDROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:')\n",
    "            print(corr_drop)\n",
    "            df = df.drop(corr_drop, axis=1)\n",
    "    # check if we need to drop symbol or not\n",
    "    if 'symbol' in df.columns: df.drop(['symbol'], axis=1, inplace=True)\n",
    "        \n",
    "    df = df.dropna()    \n",
    "    print_bold('\\nSHAPE OF DF:')\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function 'feature_engineering' performs the following:\n",
    "- Takes all of the categorical columns that were created from the 'forex factory webscrapping' and turns them into binary columns.\n",
    "    - They are separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this model, we will try multiple approaches to compare.\n",
    "1. All symbols model\n",
    "2. All symbols multi-class\n",
    "3. Single symbol with all symbols model approach\n",
    "4. Single symbol without scaling features as done in all symbols approach (to confirm if the model performs the same so that we can scale features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__rsi_rolling', 'ind__stoK_rolling']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(35213, 62)\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "df_all = feature_engineering(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__midbb_pct', 'ind__rsi_rolling', 'ind__stoK_rolling', 'ind__stochK', 'ind__surf_pct', 'move_up']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(16907, 58)\n"
     ]
    }
   ],
   "source": [
    "df_75 = feature_engineering(df.copy(), corr_co=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all of the entries that are executed on a minute divisible of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convert = df_75.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convert['min'] = df_convert.apply(lambda x: int(x.name.split()[1].split(':')[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_convert[df_convert['min']%5==0]\n",
    "df_0 = df_0.drop('min',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 58)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../data/full_data/updated_apicall_data/compiled/all_11192020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4838987488479267"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df_all['exratedif_pct']).quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "df_multi = df_75.copy()\n",
    "df_multi['dir__up_1__down_0'] = df_multi.apply(lambda x: 2 if abs(x['exratedif_pct'])<3 else x['dir__up_1__down_0'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_0 (2174, 64)\n",
      "df_1 (2636, 64)\n",
      "df_2 (2624, 64)\n",
      "df_3 (457, 64)\n",
      "df_4 (16, 64)\n",
      "df_5 (461, 64)\n"
     ]
    }
   ],
   "source": [
    "for key in dfs.keys():\n",
    "    print(key, dfs[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = dfs['df_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med', 'ind__nextimp_sent_neut']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__base_mins_to_next', 'ind__base_nextevent', 'ind__nextimp_med', 'ind__nextimp_sent_pos', 'ind__quote_mins_to_next', 'ind__rsi', 'ind__stochK']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(2547, 58)\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "sing = feature_engineering(single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med', 'ind__nextimp_sent_neut']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['Close', 'Close_High_avg', 'Close_Low_avg', 'High', 'Low', 'Open', 'ind__base_mins_to_next', 'ind__base_nextevent', 'ind__mid_bb', 'ind__nextimp_med', 'ind__nextimp_sent_pos', 'ind__quote_mins_to_next', 'ind__rsi', 'ind__stochK', 'ind__up_bb']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(2548, 60)\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "sing2 = feature_engineering(single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import estimators/algorithms to be tested for our models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# import modules for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, accuracy_score\n",
    "\n",
    "# import modules for cross validation\n",
    "from sklearn.model_selection import cross_validate, KFold  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_cls(df, get_model=False, multi=False):\n",
    "     \n",
    "    # split data --\n",
    "    X = df.drop(['exratedif_pct','dir__up_1__down_0'], axis=1)\n",
    "    y = df.dir__up_1__down_0\n",
    "    X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    # train and cross validate all models\n",
    "    cv_results, models_tbl, cv_method_tbl = [],[],[]\n",
    "    # set random state\n",
    "    rs = 1\n",
    "    # Create a list of classifiers to be tested\n",
    "    classifiers = []\n",
    "    classifiers.append(AdaBoostClassifier(RandomForestClassifier(n_estimators=100),n_estimators=100, random_state=rs))\n",
    "    classifiers.append(RandomForestClassifier(n_estimators=100, random_state=rs))\n",
    "\n",
    "    # cv method\n",
    "    cv_list = []\n",
    "    cv_list.append(KFold(n_splits=10))\n",
    "\n",
    "    # set variable for model count\n",
    "    i = 1\n",
    "    for cv_method in cv_list:\n",
    "        for cls in classifiers:\n",
    "            # perform cross validation\n",
    "            if multi: avg = 'macro'\n",
    "            else: avg='binary'\n",
    "            scoring = {'accuracy':make_scorer(accuracy_score) ,\n",
    "                       'precision':make_scorer(precision_score, average=avg),\n",
    "                       'f1':make_scorer(f1_score, average=avg)}\n",
    "            scores = cross_validate(cls, X=X_train, y=y_train, cv=cv_method, scoring=scoring, )#, n_jobs=-1)\n",
    "            # assign variable for naming classifiers\n",
    "            cls_ = str(cls).split('(')[0]\n",
    "            models_tbl.append(cls_+' - cvm_'+''.join([c for c in str(str(cv_method).split('(')[0]) if c.isupper()]))\n",
    "            cv_method_tbl.append(str(cv_method).split('(')[0])\n",
    "            cv_results.append(scores)\n",
    "            # next model\n",
    "            i+=1 \n",
    "    ## ---------------------------\n",
    "    # Create table to show results\n",
    "    fit_time, score_time = [],[]\n",
    "    accuracy, precision, f1 = [],[],[]\n",
    "\n",
    "    print_bold('\\n\\n\\nSTEP2: Model analysis...\\n\\n')\n",
    "    for n, item in enumerate(cv_results):\n",
    "        for key in item:\n",
    "            if key == 'fit_time': fit_time.append(item[key].mean())\n",
    "            elif key == 'score_time': score_time.append(item[key].mean())\n",
    "            elif 'accuracy' in key: accuracy.append(np.mean(item[key]))\n",
    "            elif 'precision' in key: precision.append(np.mean(item[key]))\n",
    "            elif 'f1' in key: f1.append(np.mean(item[key]))\n",
    "        \n",
    "    result_tbl = pd.DataFrame(data={'CV Method':cv_method_tbl, 'accuracy':accuracy, 'precision':precision, 'f1':f1,\n",
    "                                    'Fit time':fit_time, 'Score time':score_time},\n",
    "                              index=models_tbl)\n",
    "    display(result_tbl.sort_values(by='accuracy', ascending=False))\n",
    "    \n",
    "    if get_model:\n",
    "        pass\n",
    "\n",
    "def class_param_tuning(df, folds=5, estimator='adb'):\n",
    "    '''\n",
    "    Types of estimators: 'adb', 'svc'\n",
    "    '''\n",
    "    if estimator=='adb':\n",
    "        est = AdaBoostClassifier(RandomForestClassifier(random_state=1), random_state=1)\n",
    "        params = {'base_estimator__n_estimators':[200,300],\n",
    "                  'n_estimators':[300,400,500],\n",
    "                  'learning_rate':[1,0.1,0.0001]\n",
    "             }\n",
    "    elif estimator=='svc':\n",
    "        est = SVC()\n",
    "        params = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "                  'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                  'degree':[2,3,5],\n",
    "                  'kernel': ['rbf','poly','sigmoid']}  \n",
    "\n",
    "    X = df.drop(['exratedif_pct','dir__up_1__down_0'], axis=1)\n",
    "    y = df.dir__up_1__down_0\n",
    "    X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # save X columns as variable feat_single\n",
    "    feats = X.columns\n",
    "    \n",
    "    # cv method\n",
    "    kfold = KFold(folds)\n",
    "\n",
    "    #gscv = GridSearchCV(est, param_grid=params, scoring='accuracy', cv=kfold, verbose=3) #, n_jobs=-1)\n",
    "    gscv = RandomizedSearchCV(est, param_distributions=params, verbose=3, n_iter=10, scoring=['accuracy'],\n",
    "                              cv=kfold, refit='accuracy', n_jobs=-1)\n",
    "    gscv.fit(X_train, y_train)\n",
    "    print(f'\\nBest score: {gscv.best_score_}')\n",
    "    print(f'Best parameters: {gscv.best_params_}')\n",
    "\n",
    "    # create model from best parameters\n",
    "    model = est.set_params(**gscv.best_params_)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "#     df_disp = pd.DataFrame({'features':X.columns ,'importance':model.feature_importances_})\\\n",
    "#     .sort_values(by='importance', ascending=False).head(10)\n",
    "#     display(df_disp)\n",
    "    return model, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__base_mins2next', 'ind__nextimp_sent_pos', 'ind__rsi_rolling', 'ind__stoK_rolling']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(7911, 61)\n"
     ]
    }
   ],
   "source": [
    "df_feats = feature_engineering(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__midbb_pct', 'ind__rsi_rolling', 'ind__stoK_rolling', 'ind__stochK', 'ind__surf_pct', 'move_up']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(16907, 58)\n"
     ]
    }
   ],
   "source": [
    "df_feats75 = feature_engineering(df.copy(), corr_co=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  50 | elapsed:    8.4s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.5521719659650695\n",
      "Best parameters: {'n_estimators': 300, 'learning_rate': 0.0001, 'base_estimator__n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.61      0.57       466\n",
      "           1       0.57      0.50      0.53       492\n",
      "\n",
      "    accuracy                           0.55       958\n",
      "   macro avg       0.55      0.55      0.55       958\n",
      "weighted avg       0.55      0.55      0.55       958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m0,f0 = class_param_tuning(df_0, folds=5, estimator='adb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "\n",
      "\n",
      "STEP2: Model analysis...\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>Fit time</th>\n",
       "      <th>Score time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.595316</td>\n",
       "      <td>0.599810</td>\n",
       "      <td>0.563451</td>\n",
       "      <td>2.153056</td>\n",
       "      <td>0.089863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.594555</td>\n",
       "      <td>0.598728</td>\n",
       "      <td>0.561417</td>\n",
       "      <td>2.426828</td>\n",
       "      <td>0.092061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                CV Method  accuracy  precision        f1  \\\n",
       "RandomForestClassifier - cvm_KF     KFold  0.595316   0.599810  0.563451   \n",
       "AdaBoostClassifier - cvm_KF         KFold  0.594555   0.598728  0.561417   \n",
       "\n",
       "                                 Fit time  Score time  \n",
       "RandomForestClassifier - cvm_KF  2.153056    0.089863  \n",
       "AdaBoostClassifier - cvm_KF      2.426828    0.092061  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_cls(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  50 | elapsed:   49.8s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   55.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.6029237789420314\n",
      "Best parameters: {'n_estimators': 500, 'learning_rate': 0.0001, 'base_estimator__n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      2498\n",
      "           1       0.60      0.54      0.57      2575\n",
      "\n",
      "    accuracy                           0.59      5073\n",
      "   macro avg       0.59      0.59      0.59      5073\n",
      "weighted avg       0.59      0.59      0.59      5073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m1,f1 = class_param_tuning(df_all, folds=5, estimator='adb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# saving model file\n",
    "file_name = 'models/all_feats.pkl'\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(m1, file)\n",
    "    \n",
    "# write the lists of features to a file\n",
    "with open('models/all_feats__feats.txt', 'w') as filehandle:\n",
    "    for listitem in f1:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  50 | elapsed:   41.4s remaining:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   44.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.6058813587966875\n",
      "Best parameters: {'n_estimators': 500, 'learning_rate': 1, 'base_estimator__n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.65      0.62      2498\n",
      "           1       0.62      0.56      0.59      2575\n",
      "\n",
      "    accuracy                           0.60      5073\n",
      "   macro avg       0.61      0.61      0.60      5073\n",
      "weighted avg       0.61      0.60      0.60      5073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod2, feats2 = class_param_tuning(df_feats75, folds=5, estimator='adb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  50 | elapsed:   37.9s remaining:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   40.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.8565151259083995\n",
      "Best parameters: {'n_estimators': 300, 'learning_rate': 0.0001, 'base_estimator__n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.01      0.01       369\n",
      "         1.0       0.62      0.03      0.06       391\n",
      "         2.0       0.85      1.00      0.92      4313\n",
      "\n",
      "    accuracy                           0.85      5073\n",
      "   macro avg       0.56      0.35      0.33      5073\n",
      "weighted avg       0.79      0.85      0.79      5073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod5, feats5 = class_param_tuning(df_multi, folds=5, estimator='adb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# saving model file\n",
    "# file_name = 'models/multi_class.pkl'\n",
    "# with open(file_name, 'wb') as file:\n",
    "#     pickle.dump(mod5, file)\n",
    "    \n",
    "file_name = 'models/feats75.pkl'\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(mod2, file)\n",
    "    \n",
    "# file_name = 'models/all_feats.pkl'\n",
    "# with open(file_name, 'wb') as file:\n",
    "#     pickle.dump(mod4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the lists of features to a file\n",
    "# with open('models/all_feats__feats.txt', 'w') as filehandle:\n",
    "#     for listitem in feats4:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "# with open('models/multi_class__feats.txt', 'w') as filehandle:\n",
    "#     for listitem in feats5:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('models/feats75__feats.txt', 'w') as filehandle:\n",
    "    for listitem in feats2:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   32.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.6234567901234568\n",
      "Best parameters: {'base_estimator__min_samples_leaf': 1, 'base_estimator__n_estimators': 200, 'learning_rate': 1, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.64      0.61       376\n",
      "           1       0.61      0.55      0.58       389\n",
      "\n",
      "    accuracy                           0.59       765\n",
      "   macro avg       0.60      0.60      0.59       765\n",
      "weighted avg       0.60      0.59      0.59       765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod, feats = class_param_tuning(sing, folds=3, estimator='adb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.628154795288839\n",
      "Best parameters: {'base_estimator__min_samples_leaf': 1, 'base_estimator__n_estimators': 200, 'learning_rate': 1, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.64      0.63       390\n",
      "           1       0.61      0.57      0.59       375\n",
      "\n",
      "    accuracy                           0.61       765\n",
      "   macro avg       0.61      0.61      0.61       765\n",
      "weighted avg       0.61      0.61      0.61       765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sing2 = sing2.drop('expiration',axis=1)\n",
    "mod, feats = class_param_tuning(sing2, folds=10, estimator='adb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- no big difference between training models based on score and feature training, we will go with the all symbols feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
