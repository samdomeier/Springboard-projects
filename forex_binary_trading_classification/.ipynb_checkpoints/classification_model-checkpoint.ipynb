{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreign Exchange Binary Classification Model for Trading\n",
    "This notebook includes all of the feature engineering and modeling for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import modf\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# graphing modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\ext_AUDNZD_train_3_min_pos.csv (2174, 64)\n",
      "data\\ext_AUDUSD_train_3_min_pos.csv (2636, 64)\n",
      "data\\ext_EURCAD_train_3_min_pos.csv (2624, 64)\n",
      "data\\ext_EURGBP_train_3_min_pos.csv (1728, 64)\n",
      "data\\ext_EURJPY_train_3_min_pos.csv (7864, 64)\n",
      "data\\ext_GBPAUD_train_3_min_pos.csv (5565, 64)\n",
      "data\\ext_GBPCHF_train_3_min_pos.csv (1562, 64)\n",
      "data\\ext_GBPUSD_train_3_min_pos.csv (1306, 64)\n",
      "data\\ext_USDAUD_train_3_min_pos.csv (6180, 64)\n",
      "data\\ext_USDCAD_train_3_min_pos.csv (5923, 64)\n",
      "data\\ext_USDJPY_train_3_min_pos.csv (4235, 64)\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/*.csv'\n",
    "for file in glob.glob(data_path):\n",
    "    add = pd.read_csv(file, index_col='date')\n",
    "    print(file, add.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>symbol</th>\n",
       "      <th>Close_Low_avg</th>\n",
       "      <th>Close_High_avg</th>\n",
       "      <th>move_up</th>\n",
       "      <th>wick_high</th>\n",
       "      <th>wick_low</th>\n",
       "      <th>ind__stochK</th>\n",
       "      <th>ind__rsi</th>\n",
       "      <th>ind__mid_bb</th>\n",
       "      <th>ind__up_bb</th>\n",
       "      <th>ind__low_bb</th>\n",
       "      <th>ind__bb_dif</th>\n",
       "      <th>ind__volatile</th>\n",
       "      <th>ind__rsi_accel</th>\n",
       "      <th>ind__rsi_rolling</th>\n",
       "      <th>ind__stoK_accel</th>\n",
       "      <th>ind__stoK_rolling</th>\n",
       "      <th>ind__outside_up</th>\n",
       "      <th>ind__outside_low</th>\n",
       "      <th>ind__surf_pct</th>\n",
       "      <th>spread</th>\n",
       "      <th>bid_diff</th>\n",
       "      <th>ask_diff</th>\n",
       "      <th>ind__full_near_res</th>\n",
       "      <th>ind__full_near_sup</th>\n",
       "      <th>ind__2_3_near_res</th>\n",
       "      <th>ind__2_3_near_sup</th>\n",
       "      <th>ind__1_3_near_res</th>\n",
       "      <th>ind__1_3_near_sup</th>\n",
       "      <th>ind__base_strength</th>\n",
       "      <th>ind__quote_strength</th>\n",
       "      <th>ind__strength_ratio</th>\n",
       "      <th>ind__BS_diff</th>\n",
       "      <th>ind__QS_diff</th>\n",
       "      <th>ind__BS_pastchg</th>\n",
       "      <th>ind__QS_pastchg</th>\n",
       "      <th>ind__midbb_slope</th>\n",
       "      <th>ind__trend_residuals</th>\n",
       "      <th>ind__pub_twit_pol</th>\n",
       "      <th>ind__trad_twit_pol</th>\n",
       "      <th>ind__numtweets_1min</th>\n",
       "      <th>ind__base_mins_to_next</th>\n",
       "      <th>ind__quote_mins_to_next</th>\n",
       "      <th>ind__base_recentevent</th>\n",
       "      <th>ind__quote_recentevent</th>\n",
       "      <th>ind__base_numevents</th>\n",
       "      <th>ind__quote_numevents</th>\n",
       "      <th>ind__base_foreimpact</th>\n",
       "      <th>ind__base_ffalert</th>\n",
       "      <th>ind__base_nextevent_sent</th>\n",
       "      <th>ind__quote_foreimpact</th>\n",
       "      <th>ind__quote_ffalert</th>\n",
       "      <th>ind__quote_nextevent_sent</th>\n",
       "      <th>ind__base_pastimpact</th>\n",
       "      <th>ind__base_pastevent_sent</th>\n",
       "      <th>ind__quote_pastimpact</th>\n",
       "      <th>ind__quote_pastevent_sent</th>\n",
       "      <th>expiration</th>\n",
       "      <th>exch_rate_dif</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:03:00</th>\n",
       "      <td>1.06000</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>1.06009</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059895</td>\n",
       "      <td>1.060095</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>65.000</td>\n",
       "      <td>46.537</td>\n",
       "      <td>1.060094</td>\n",
       "      <td>1.060314</td>\n",
       "      <td>1.059874</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.464</td>\n",
       "      <td>47.2690</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>65.8335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511851</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000e-06</td>\n",
       "      <td>3.040000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:06:00</td>\n",
       "      <td>-0.00013</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:04:00</th>\n",
       "      <td>1.06009</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>1.06003</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059865</td>\n",
       "      <td>1.060065</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>64.167</td>\n",
       "      <td>37.875</td>\n",
       "      <td>1.060096</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>1.059879</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.662</td>\n",
       "      <td>42.2060</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>64.5835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453719</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000e-06</td>\n",
       "      <td>1.870000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:07:00</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:05:00</th>\n",
       "      <td>1.06003</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0596</td>\n",
       "      <td>1.05992</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059760</td>\n",
       "      <td>1.060010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>32.500</td>\n",
       "      <td>26.549</td>\n",
       "      <td>1.060097</td>\n",
       "      <td>1.060311</td>\n",
       "      <td>1.059883</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.326</td>\n",
       "      <td>32.2120</td>\n",
       "      <td>-31.667</td>\n",
       "      <td>48.3335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308428</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000e-06</td>\n",
       "      <td>2.590000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:08:00</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:06:00</th>\n",
       "      <td>1.05992</td>\n",
       "      <td>1.0601</td>\n",
       "      <td>1.0596</td>\n",
       "      <td>1.05996</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059780</td>\n",
       "      <td>1.060030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>11.111</td>\n",
       "      <td>35.338</td>\n",
       "      <td>1.060095</td>\n",
       "      <td>1.060313</td>\n",
       "      <td>1.059877</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0</td>\n",
       "      <td>8.789</td>\n",
       "      <td>30.9435</td>\n",
       "      <td>-21.389</td>\n",
       "      <td>21.8055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208247</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.000000e-07</td>\n",
       "      <td>4.750000e-09</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:09:00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-16 21:07:00</th>\n",
       "      <td>1.05996</td>\n",
       "      <td>1.0600</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>1.06001</td>\n",
       "      <td>AUDNZD</td>\n",
       "      <td>1.059855</td>\n",
       "      <td>1.060005</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>37.582</td>\n",
       "      <td>45.523</td>\n",
       "      <td>1.060100</td>\n",
       "      <td>1.060303</td>\n",
       "      <td>1.059897</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0</td>\n",
       "      <td>10.185</td>\n",
       "      <td>40.4305</td>\n",
       "      <td>26.471</td>\n",
       "      <td>24.3465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185047</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000e-06</td>\n",
       "      <td>1.179000e-08</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>high_none</td>\n",
       "      <td>none</td>\n",
       "      <td>high_negative</td>\n",
       "      <td>multiple_low_right</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>multiple_low_neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>2020-11-16 21:10:00</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Open    High     Low    Close  symbol  Close_Low_avg  \\\n",
       "date                                                                           \n",
       "2020-11-16 21:03:00  1.06000  1.0601  1.0597  1.06009  AUDNZD       1.059895   \n",
       "2020-11-16 21:04:00  1.06009  1.0601  1.0597  1.06003  AUDNZD       1.059865   \n",
       "2020-11-16 21:05:00  1.06003  1.0601  1.0596  1.05992  AUDNZD       1.059760   \n",
       "2020-11-16 21:06:00  1.05992  1.0601  1.0596  1.05996  AUDNZD       1.059780   \n",
       "2020-11-16 21:07:00  1.05996  1.0600  1.0597  1.06001  AUDNZD       1.059855   \n",
       "\n",
       "                     Close_High_avg  move_up  wick_high  wick_low  \\\n",
       "date                                                                \n",
       "2020-11-16 21:03:00        1.060095        1    0.00001   0.00030   \n",
       "2020-11-16 21:04:00        1.060065        0    0.00001   0.00033   \n",
       "2020-11-16 21:05:00        1.060010        0    0.00007   0.00032   \n",
       "2020-11-16 21:06:00        1.060030        1    0.00014   0.00032   \n",
       "2020-11-16 21:07:00        1.060005        1   -0.00001   0.00026   \n",
       "\n",
       "                     ind__stochK  ind__rsi  ind__mid_bb  ind__up_bb  \\\n",
       "date                                                                  \n",
       "2020-11-16 21:03:00       65.000    46.537     1.060094    1.060314   \n",
       "2020-11-16 21:04:00       64.167    37.875     1.060096    1.060313   \n",
       "2020-11-16 21:05:00       32.500    26.549     1.060097    1.060311   \n",
       "2020-11-16 21:06:00       11.111    35.338     1.060095    1.060313   \n",
       "2020-11-16 21:07:00       37.582    45.523     1.060100    1.060303   \n",
       "\n",
       "                     ind__low_bb  ind__bb_dif  ind__volatile  ind__rsi_accel  \\\n",
       "date                                                                           \n",
       "2020-11-16 21:03:00     1.059874     0.000440              0          -1.464   \n",
       "2020-11-16 21:04:00     1.059879     0.000434              0          -8.662   \n",
       "2020-11-16 21:05:00     1.059883     0.000428              0         -11.326   \n",
       "2020-11-16 21:06:00     1.059877     0.000436              0           8.789   \n",
       "2020-11-16 21:07:00     1.059897     0.000406              0          10.185   \n",
       "\n",
       "                     ind__rsi_rolling  ind__stoK_accel  ind__stoK_rolling  \\\n",
       "date                                                                        \n",
       "2020-11-16 21:03:00           47.2690           -1.667            65.8335   \n",
       "2020-11-16 21:04:00           42.2060           -0.833            64.5835   \n",
       "2020-11-16 21:05:00           32.2120          -31.667            48.3335   \n",
       "2020-11-16 21:06:00           30.9435          -21.389            21.8055   \n",
       "2020-11-16 21:07:00           40.4305           26.471            24.3465   \n",
       "\n",
       "                     ind__outside_up  ind__outside_low  ind__surf_pct  \\\n",
       "date                                                                    \n",
       "2020-11-16 21:03:00                0                 0       0.511851   \n",
       "2020-11-16 21:04:00                0                 0       0.453719   \n",
       "2020-11-16 21:05:00                0                 0       0.308428   \n",
       "2020-11-16 21:06:00                0                 0       0.208247   \n",
       "2020-11-16 21:07:00                0                 0       0.185047   \n",
       "\n",
       "                       spread  bid_diff  ask_diff  ind__full_near_res  \\\n",
       "date                                                                    \n",
       "2020-11-16 21:03:00  0.000080  0.000048  0.000032                   0   \n",
       "2020-11-16 21:04:00  0.000041  0.000023  0.000018                   0   \n",
       "2020-11-16 21:05:00  0.000060  0.000030  0.000030                   0   \n",
       "2020-11-16 21:06:00  0.000061  0.000041  0.000020                   0   \n",
       "2020-11-16 21:07:00  0.000089  0.000044  0.000044                   0   \n",
       "\n",
       "                     ind__full_near_sup  ind__2_3_near_res  ind__2_3_near_sup  \\\n",
       "date                                                                            \n",
       "2020-11-16 21:03:00                   0                  0                  1   \n",
       "2020-11-16 21:04:00                   0                  0                  0   \n",
       "2020-11-16 21:05:00                   0                  0                  0   \n",
       "2020-11-16 21:06:00                   0                  0                  0   \n",
       "2020-11-16 21:07:00                   0                  0                  1   \n",
       "\n",
       "                     ind__1_3_near_res  ind__1_3_near_sup  ind__base_strength  \\\n",
       "date                                                                            \n",
       "2020-11-16 21:03:00                  0                  0                   6   \n",
       "2020-11-16 21:04:00                  0                  0                   6   \n",
       "2020-11-16 21:05:00                  0                  0                   6   \n",
       "2020-11-16 21:06:00                  0                  1                   6   \n",
       "2020-11-16 21:07:00                  0                  1                   6   \n",
       "\n",
       "                     ind__quote_strength  ind__strength_ratio  ind__BS_diff  \\\n",
       "date                                                                          \n",
       "2020-11-16 21:03:00                    6                  1.0             0   \n",
       "2020-11-16 21:04:00                    6                  1.0             0   \n",
       "2020-11-16 21:05:00                    6                  1.0             0   \n",
       "2020-11-16 21:06:00                    6                  1.0             0   \n",
       "2020-11-16 21:07:00                    6                  1.0             0   \n",
       "\n",
       "                     ind__QS_diff  ind__BS_pastchg  ind__QS_pastchg  \\\n",
       "date                                                                  \n",
       "2020-11-16 21:03:00             0                0                0   \n",
       "2020-11-16 21:04:00             0                0                0   \n",
       "2020-11-16 21:05:00             0                0                0   \n",
       "2020-11-16 21:06:00             0                0                0   \n",
       "2020-11-16 21:07:00             0                0                0   \n",
       "\n",
       "                     ind__midbb_slope  ind__trend_residuals  \\\n",
       "date                                                          \n",
       "2020-11-16 21:03:00      7.000000e-06          3.040000e-09   \n",
       "2020-11-16 21:04:00      3.000000e-06          1.870000e-09   \n",
       "2020-11-16 21:05:00      1.500000e-06          2.590000e-09   \n",
       "2020-11-16 21:06:00     -5.000000e-07          4.750000e-09   \n",
       "2020-11-16 21:07:00      1.500000e-06          1.179000e-08   \n",
       "\n",
       "                     ind__pub_twit_pol  ind__trad_twit_pol  \\\n",
       "date                                                         \n",
       "2020-11-16 21:03:00          -0.142857                 1.0   \n",
       "2020-11-16 21:04:00          -0.142857                 1.0   \n",
       "2020-11-16 21:05:00          -0.142857                 1.0   \n",
       "2020-11-16 21:06:00          -0.142857                 1.0   \n",
       "2020-11-16 21:07:00          -0.142857                 1.0   \n",
       "\n",
       "                     ind__numtweets_1min ind__base_mins_to_next  \\\n",
       "date                                                              \n",
       "2020-11-16 21:03:00                    0                     56   \n",
       "2020-11-16 21:04:00                    0                     55   \n",
       "2020-11-16 21:05:00                    0                     54   \n",
       "2020-11-16 21:06:00                    0                     53   \n",
       "2020-11-16 21:07:00                    0                     52   \n",
       "\n",
       "                    ind__quote_mins_to_next  ind__base_recentevent  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00                    41.0                      0   \n",
       "2020-11-16 21:04:00                    40.0                      0   \n",
       "2020-11-16 21:05:00                    39.0                      0   \n",
       "2020-11-16 21:06:00                    38.0                      0   \n",
       "2020-11-16 21:07:00                    37.0                      0   \n",
       "\n",
       "                     ind__quote_recentevent  ind__base_numevents  \\\n",
       "date                                                               \n",
       "2020-11-16 21:03:00                       0                    5   \n",
       "2020-11-16 21:04:00                       0                    5   \n",
       "2020-11-16 21:05:00                       0                    5   \n",
       "2020-11-16 21:06:00                       0                    5   \n",
       "2020-11-16 21:07:00                       0                    5   \n",
       "\n",
       "                     ind__quote_numevents ind__base_foreimpact  \\\n",
       "date                                                             \n",
       "2020-11-16 21:03:00                     2            high_none   \n",
       "2020-11-16 21:04:00                     2            high_none   \n",
       "2020-11-16 21:05:00                     2            high_none   \n",
       "2020-11-16 21:06:00                     2            high_none   \n",
       "2020-11-16 21:07:00                     2            high_none   \n",
       "\n",
       "                    ind__base_ffalert ind__base_nextevent_sent  \\\n",
       "date                                                             \n",
       "2020-11-16 21:03:00              none            high_negative   \n",
       "2020-11-16 21:04:00              none            high_negative   \n",
       "2020-11-16 21:05:00              none            high_negative   \n",
       "2020-11-16 21:06:00              none            high_negative   \n",
       "2020-11-16 21:07:00              none            high_negative   \n",
       "\n",
       "                    ind__quote_foreimpact ind__quote_ffalert  \\\n",
       "date                                                           \n",
       "2020-11-16 21:03:00    multiple_low_right               none   \n",
       "2020-11-16 21:04:00    multiple_low_right               none   \n",
       "2020-11-16 21:05:00    multiple_low_right               none   \n",
       "2020-11-16 21:06:00    multiple_low_right               none   \n",
       "2020-11-16 21:07:00    multiple_low_right               none   \n",
       "\n",
       "                    ind__quote_nextevent_sent ind__base_pastimpact  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:04:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:05:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:06:00      multiple_low_neutral                 none   \n",
       "2020-11-16 21:07:00      multiple_low_neutral                 none   \n",
       "\n",
       "                    ind__base_pastevent_sent ind__quote_pastimpact  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:04:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:05:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:06:00     multiple_low_neutral                  none   \n",
       "2020-11-16 21:07:00     multiple_low_neutral                  none   \n",
       "\n",
       "                    ind__quote_pastevent_sent           expiration  \\\n",
       "date                                                                 \n",
       "2020-11-16 21:03:00                      none  2020-11-16 21:06:00   \n",
       "2020-11-16 21:04:00                      none  2020-11-16 21:07:00   \n",
       "2020-11-16 21:05:00                      none  2020-11-16 21:08:00   \n",
       "2020-11-16 21:06:00                      none  2020-11-16 21:09:00   \n",
       "2020-11-16 21:07:00                      none  2020-11-16 21:10:00   \n",
       "\n",
       "                     exch_rate_dif direction  \n",
       "date                                          \n",
       "2020-11-16 21:03:00       -0.00013      down  \n",
       "2020-11-16 21:04:00       -0.00002      down  \n",
       "2020-11-16 21:05:00        0.00024        up  \n",
       "2020-11-16 21:06:00        0.00000       NaN  \n",
       "2020-11-16 21:07:00       -0.00002      down  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = {}\n",
    "df = pd.DataFrame()\n",
    "for i,file in enumerate(glob.glob(data_path)):\n",
    "    add = pd.read_csv(file, index_col='date')\n",
    "    dfs['df_'+str(i)] = add\n",
    "    df = pd.concat([df, add])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41797, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close_Low_avg</th>\n",
       "      <th>Close_High_avg</th>\n",
       "      <th>move_up</th>\n",
       "      <th>wick_high</th>\n",
       "      <th>wick_low</th>\n",
       "      <th>ind__stochK</th>\n",
       "      <th>ind__rsi</th>\n",
       "      <th>ind__mid_bb</th>\n",
       "      <th>ind__up_bb</th>\n",
       "      <th>ind__low_bb</th>\n",
       "      <th>ind__bb_dif</th>\n",
       "      <th>ind__volatile</th>\n",
       "      <th>ind__rsi_accel</th>\n",
       "      <th>ind__rsi_rolling</th>\n",
       "      <th>ind__stoK_accel</th>\n",
       "      <th>ind__stoK_rolling</th>\n",
       "      <th>ind__outside_up</th>\n",
       "      <th>ind__outside_low</th>\n",
       "      <th>ind__surf_pct</th>\n",
       "      <th>spread</th>\n",
       "      <th>bid_diff</th>\n",
       "      <th>ask_diff</th>\n",
       "      <th>ind__full_near_res</th>\n",
       "      <th>ind__full_near_sup</th>\n",
       "      <th>ind__2_3_near_res</th>\n",
       "      <th>ind__2_3_near_sup</th>\n",
       "      <th>ind__1_3_near_res</th>\n",
       "      <th>ind__1_3_near_sup</th>\n",
       "      <th>ind__base_strength</th>\n",
       "      <th>ind__quote_strength</th>\n",
       "      <th>ind__strength_ratio</th>\n",
       "      <th>ind__BS_diff</th>\n",
       "      <th>ind__QS_diff</th>\n",
       "      <th>ind__BS_pastchg</th>\n",
       "      <th>ind__QS_pastchg</th>\n",
       "      <th>ind__midbb_slope</th>\n",
       "      <th>ind__trend_residuals</th>\n",
       "      <th>ind__pub_twit_pol</th>\n",
       "      <th>ind__trad_twit_pol</th>\n",
       "      <th>ind__numtweets_1min</th>\n",
       "      <th>ind__base_recentevent</th>\n",
       "      <th>ind__quote_recentevent</th>\n",
       "      <th>ind__base_numevents</th>\n",
       "      <th>ind__quote_numevents</th>\n",
       "      <th>exch_rate_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.00000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>4.179700e+04</td>\n",
       "      <td>4.179700e+04</td>\n",
       "      <td>36493.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "      <td>41797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.126048</td>\n",
       "      <td>35.129885</td>\n",
       "      <td>35.116332</td>\n",
       "      <td>35.126100</td>\n",
       "      <td>35.121216</td>\n",
       "      <td>35.127992</td>\n",
       "      <td>0.479269</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>51.277724</td>\n",
       "      <td>49.959910</td>\n",
       "      <td>35.125582</td>\n",
       "      <td>35.136788</td>\n",
       "      <td>35.114376</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.245592</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>49.958861</td>\n",
       "      <td>0.015565</td>\n",
       "      <td>51.269941</td>\n",
       "      <td>0.039955</td>\n",
       "      <td>0.047850</td>\n",
       "      <td>0.498260</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.239778</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.223485</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.204704</td>\n",
       "      <td>0.196306</td>\n",
       "      <td>3.149819</td>\n",
       "      <td>3.018470</td>\n",
       "      <td>1.458237</td>\n",
       "      <td>-0.000550</td>\n",
       "      <td>-0.00012</td>\n",
       "      <td>0.025863</td>\n",
       "      <td>0.029213</td>\n",
       "      <td>5.654986e-05</td>\n",
       "      <td>8.866025e-05</td>\n",
       "      <td>0.093566</td>\n",
       "      <td>-0.204974</td>\n",
       "      <td>0.381606</td>\n",
       "      <td>0.096083</td>\n",
       "      <td>0.050889</td>\n",
       "      <td>4.451994</td>\n",
       "      <td>2.120990</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>53.215940</td>\n",
       "      <td>53.221795</td>\n",
       "      <td>53.201167</td>\n",
       "      <td>53.216032</td>\n",
       "      <td>53.208599</td>\n",
       "      <td>53.218913</td>\n",
       "      <td>0.499576</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.013856</td>\n",
       "      <td>33.691517</td>\n",
       "      <td>16.204582</td>\n",
       "      <td>53.215120</td>\n",
       "      <td>53.232096</td>\n",
       "      <td>53.198147</td>\n",
       "      <td>0.041234</td>\n",
       "      <td>0.430443</td>\n",
       "      <td>12.816258</td>\n",
       "      <td>14.884937</td>\n",
       "      <td>25.068982</td>\n",
       "      <td>31.269449</td>\n",
       "      <td>0.195856</td>\n",
       "      <td>0.213452</td>\n",
       "      <td>0.277337</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>0.406418</td>\n",
       "      <td>0.416586</td>\n",
       "      <td>0.412815</td>\n",
       "      <td>0.403490</td>\n",
       "      <td>0.397207</td>\n",
       "      <td>1.591439</td>\n",
       "      <td>1.729962</td>\n",
       "      <td>1.441735</td>\n",
       "      <td>0.469089</td>\n",
       "      <td>0.48843</td>\n",
       "      <td>0.158729</td>\n",
       "      <td>0.168404</td>\n",
       "      <td>1.316100e-03</td>\n",
       "      <td>2.906878e-04</td>\n",
       "      <td>0.293084</td>\n",
       "      <td>0.530156</td>\n",
       "      <td>2.478906</td>\n",
       "      <td>0.294709</td>\n",
       "      <td>0.219773</td>\n",
       "      <td>4.849597</td>\n",
       "      <td>2.551035</td>\n",
       "      <td>0.011670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.727260</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>0.727260</td>\n",
       "      <td>0.727115</td>\n",
       "      <td>0.727425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.021000</td>\n",
       "      <td>-0.029000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.534000</td>\n",
       "      <td>0.727666</td>\n",
       "      <td>0.727901</td>\n",
       "      <td>0.727015</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.862000</td>\n",
       "      <td>1.833500</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.328497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-6.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.227500e-02</td>\n",
       "      <td>6.675654e-32</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.301500</td>\n",
       "      <td>1.301700</td>\n",
       "      <td>1.301100</td>\n",
       "      <td>1.301500</td>\n",
       "      <td>1.301300</td>\n",
       "      <td>1.301600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>21.429000</td>\n",
       "      <td>38.970000</td>\n",
       "      <td>1.301530</td>\n",
       "      <td>1.302062</td>\n",
       "      <td>1.300969</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.355000</td>\n",
       "      <td>40.308500</td>\n",
       "      <td>-15.574000</td>\n",
       "      <td>24.576000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275184</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.750000e-05</td>\n",
       "      <td>1.996000e-08</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359600</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.359515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.377000</td>\n",
       "      <td>1.359352</td>\n",
       "      <td>1.360176</td>\n",
       "      <td>1.358642</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.118000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.499500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499580</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.979041e-16</td>\n",
       "      <td>8.428000e-08</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.399000</td>\n",
       "      <td>104.407000</td>\n",
       "      <td>104.370000</td>\n",
       "      <td>104.399000</td>\n",
       "      <td>104.384500</td>\n",
       "      <td>104.403000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>61.027000</td>\n",
       "      <td>104.400350</td>\n",
       "      <td>104.430120</td>\n",
       "      <td>104.365925</td>\n",
       "      <td>0.036704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.283000</td>\n",
       "      <td>59.696500</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>78.316000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722353</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000e-05</td>\n",
       "      <td>3.670000e-05</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>126.672000</td>\n",
       "      <td>126.682000</td>\n",
       "      <td>126.630000</td>\n",
       "      <td>126.672000</td>\n",
       "      <td>126.644500</td>\n",
       "      <td>126.672000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.383000</td>\n",
       "      <td>126.631800</td>\n",
       "      <td>126.716502</td>\n",
       "      <td>126.608901</td>\n",
       "      <td>0.461340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.724000</td>\n",
       "      <td>96.789500</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.284345</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.785000e-02</td>\n",
       "      <td>1.046520e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close  Close_Low_avg  \\\n",
       "count  41797.000000  41797.000000  41797.000000  41797.000000   41797.000000   \n",
       "mean      35.126048     35.129885     35.116332     35.126100      35.121216   \n",
       "std       53.215940     53.221795     53.201167     53.216032      53.208599   \n",
       "min        0.727260      0.727500      0.726800      0.727260       0.727115   \n",
       "25%        1.301500      1.301700      1.301100      1.301500       1.301300   \n",
       "50%        1.359400      1.359600      1.359400      1.359400       1.359400   \n",
       "75%      104.399000    104.407000    104.370000    104.399000     104.384500   \n",
       "max      126.672000    126.682000    126.630000    126.672000     126.644500   \n",
       "\n",
       "       Close_High_avg       move_up     wick_high      wick_low   ind__stochK  \\\n",
       "count    41797.000000  41797.000000  41797.000000  41797.000000  41797.000000   \n",
       "mean        35.127992      0.479269      0.002248      0.008179     51.277724   \n",
       "std         53.218913      0.499576      0.004989      0.013856     33.691517   \n",
       "min          0.727425      0.000000     -0.021000     -0.029000      0.000000   \n",
       "25%          1.301600      0.000000      0.000030      0.000200     21.429000   \n",
       "50%          1.359515      0.000000      0.000100      0.000380     50.000000   \n",
       "75%        104.403000      1.000000      0.002000      0.017000     82.500000   \n",
       "max        126.672000      1.000000      0.075000      0.112000    100.000000   \n",
       "\n",
       "           ind__rsi   ind__mid_bb    ind__up_bb   ind__low_bb   ind__bb_dif  \\\n",
       "count  41797.000000  41797.000000  41797.000000  41797.000000  41797.000000   \n",
       "mean      49.959910     35.125582     35.136788     35.114376      0.022411   \n",
       "std       16.204582     53.215120     53.232096     53.198147      0.041234   \n",
       "min        1.534000      0.727666      0.727901      0.727015      0.000104   \n",
       "25%       38.970000      1.301530      1.302062      1.300969      0.000734   \n",
       "50%       50.377000      1.359352      1.360176      1.358642      0.001224   \n",
       "75%       61.027000    104.400350    104.430120    104.365925      0.036704   \n",
       "max       97.383000    126.631800    126.716502    126.608901      0.461340   \n",
       "\n",
       "       ind__volatile  ind__rsi_accel  ind__rsi_rolling  ind__stoK_accel  \\\n",
       "count   41797.000000    41797.000000      41797.000000     41797.000000   \n",
       "mean        0.245592        0.002098         49.958861         0.015565   \n",
       "std         0.430443       12.816258         14.884937        25.068982   \n",
       "min         0.000000      -56.862000          1.833500       -50.000000   \n",
       "25%         0.000000       -8.355000         40.308500       -15.574000   \n",
       "50%         0.000000        0.000000         50.118000         0.000000   \n",
       "75%         0.000000        8.283000         59.696500        15.625000   \n",
       "max         1.000000       56.724000         96.789500        50.000000   \n",
       "\n",
       "       ind__stoK_rolling  ind__outside_up  ind__outside_low  ind__surf_pct  \\\n",
       "count       41797.000000     41797.000000      41797.000000   41797.000000   \n",
       "mean           51.269941         0.039955          0.047850       0.498260   \n",
       "std            31.269449         0.195856          0.213452       0.277337   \n",
       "min             0.000000         0.000000          0.000000      -0.328497   \n",
       "25%            24.576000         0.000000          0.000000       0.275184   \n",
       "50%            51.499500         0.000000          0.000000       0.499580   \n",
       "75%            78.316000         0.000000          0.000000       0.722353   \n",
       "max           100.000000         1.000000          1.000000       1.284345   \n",
       "\n",
       "             spread      bid_diff      ask_diff  ind__full_near_res  \\\n",
       "count  41797.000000  41797.000000  41797.000000        41797.000000   \n",
       "mean       0.001756      0.000875      0.000881            0.239778   \n",
       "std        0.002972      0.001613      0.001622            0.426953   \n",
       "min        0.000000      0.000000      0.000000            0.000000   \n",
       "25%        0.000051      0.000022      0.000022            0.000000   \n",
       "50%        0.000082      0.000045      0.000045            0.000000   \n",
       "75%        0.003006      0.000803      0.000814            0.000000   \n",
       "max        0.012637      0.006319      0.006319            1.000000   \n",
       "\n",
       "       ind__full_near_sup  ind__2_3_near_res  ind__2_3_near_sup  \\\n",
       "count        41797.000000       41797.000000       41797.000000   \n",
       "mean             0.208747           0.223485           0.217886   \n",
       "std              0.406418           0.416586           0.412815   \n",
       "min              0.000000           0.000000           0.000000   \n",
       "25%              0.000000           0.000000           0.000000   \n",
       "50%              0.000000           0.000000           0.000000   \n",
       "75%              0.000000           0.000000           0.000000   \n",
       "max              1.000000           1.000000           1.000000   \n",
       "\n",
       "       ind__1_3_near_res  ind__1_3_near_sup  ind__base_strength  \\\n",
       "count       41797.000000       41797.000000        41797.000000   \n",
       "mean            0.204704           0.196306            3.149819   \n",
       "std             0.403490           0.397207            1.591439   \n",
       "min             0.000000           0.000000            0.000000   \n",
       "25%             0.000000           0.000000            2.000000   \n",
       "50%             0.000000           0.000000            3.000000   \n",
       "75%             0.000000           0.000000            4.000000   \n",
       "max             1.000000           1.000000            6.000000   \n",
       "\n",
       "       ind__quote_strength  ind__strength_ratio  ind__BS_diff  ind__QS_diff  \\\n",
       "count         41797.000000         41797.000000  41797.000000   41797.00000   \n",
       "mean              3.018470             1.458237     -0.000550      -0.00012   \n",
       "std               1.729962             1.441735      0.469089       0.48843   \n",
       "min               0.000000             0.000000     -6.000000      -6.00000   \n",
       "25%               1.000000             0.400000      0.000000       0.00000   \n",
       "50%               3.000000             1.000000      0.000000       0.00000   \n",
       "75%               4.000000             2.000000      0.000000       0.00000   \n",
       "max               6.000000             6.000000      6.000000       6.00000   \n",
       "\n",
       "       ind__BS_pastchg  ind__QS_pastchg  ind__midbb_slope  \\\n",
       "count     41797.000000     41797.000000      4.179700e+04   \n",
       "mean          0.025863         0.029213      5.654986e-05   \n",
       "std           0.158729         0.168404      1.316100e-03   \n",
       "min           0.000000         0.000000     -1.227500e-02   \n",
       "25%           0.000000         0.000000     -2.750000e-05   \n",
       "50%           0.000000         0.000000     -2.979041e-16   \n",
       "75%           0.000000         0.000000      2.900000e-05   \n",
       "max           1.000000         1.000000      1.785000e-02   \n",
       "\n",
       "       ind__trend_residuals  ind__pub_twit_pol  ind__trad_twit_pol  \\\n",
       "count          4.179700e+04       36493.000000        41797.000000   \n",
       "mean           8.866025e-05           0.093566           -0.204974   \n",
       "std            2.906878e-04           0.293084            0.530156   \n",
       "min            6.675654e-32          -1.000000           -1.000000   \n",
       "25%            1.996000e-08          -0.033333           -0.647059   \n",
       "50%            8.428000e-08           0.066667            0.000000   \n",
       "75%            3.670000e-05           0.228571            0.000000   \n",
       "max            1.046520e-02           1.000000            1.000000   \n",
       "\n",
       "       ind__numtweets_1min  ind__base_recentevent  ind__quote_recentevent  \\\n",
       "count         41797.000000           41797.000000            41797.000000   \n",
       "mean              0.381606               0.096083                0.050889   \n",
       "std               2.478906               0.294709                0.219773   \n",
       "min               0.000000               0.000000                0.000000   \n",
       "25%               0.000000               0.000000                0.000000   \n",
       "50%               0.000000               0.000000                0.000000   \n",
       "75%               0.000000               0.000000                0.000000   \n",
       "max             192.000000               1.000000                1.000000   \n",
       "\n",
       "       ind__base_numevents  ind__quote_numevents  exch_rate_dif  \n",
       "count         41797.000000          41797.000000   41797.000000  \n",
       "mean              4.451994              2.120990       0.000168  \n",
       "std               4.849597              2.551035       0.011670  \n",
       "min               0.000000              0.000000      -0.176000  \n",
       "25%               0.000000              0.000000      -0.000270  \n",
       "50%               3.000000              1.000000       0.000000  \n",
       "75%               6.000000              3.000000       0.000290  \n",
       "max              20.000000             14.000000       0.143000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows in the dataframe above were created using the \"forex_datamining\" notebook. The functions below will prepare the columns for model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "For the feature engineering portion, I am taking the local database that I've created and creating specific features that I want to use for this project. Specifically, we are taking the event information created from the funcitons mining from Forex Factory, and creating features based on these results.\n",
    "- In the feature engineering function, we provide a variable that filters out features based on correlation to each other.\n",
    "- Since the database is locally created, I could continue to mine data but with this comes drawbacks. Since I was only able to allow this datamining to run for a short period of time (roughly 24 hours over a 2 week period), my data isn't as diverse as I would have liked. Because of this, I need to filter out features due to lack of variance. For example, I will eliminating a categorical column if more than 70% of the values are the same.\n",
    "- For modeling purposes, we will be considering different correlation coefficients to filter out features. The following will be used:\n",
    "    - 0.9, 0.75, and 0.6 (so if a feature has a 0.9 correlation coefficient to another feature, I will only keep one)\n",
    "    - Because a lot of these features are derived from price action, they become highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bold(_str):\n",
    "    print(\"\\033[1m\" + _str + \"\\033[0m\")\n",
    "    \n",
    "\n",
    "def get_event_info(base_event, quote_event, level=None, outcome=None):\n",
    "    '''\n",
    "    - level - set to 'high','medium', or 'low'\n",
    "    - outcome - if None, then returning count of events or number of events with FF Alerts\n",
    "        - if assigned value, then returning the result actual, forecast, or sentiment\n",
    "            - this will be based on the input 'base_event','quote_event' arguments\n",
    "    '''\n",
    "    results = {'high':[],'medium':[],'low':[]}\n",
    "    for event in [base_event, quote_event]:\n",
    "        if event == 'none':\n",
    "            # all results stay empty\n",
    "            pass\n",
    "        elif 'multiple' in event:\n",
    "            # what ever impact and result is given, return 2 times that\n",
    "            items = event.split('_')\n",
    "            for key in results.keys():\n",
    "                if items[1]==key:\n",
    "                    # append this value twice for having 'multiple' same event types and results\n",
    "                    results[key].append(items[2])\n",
    "                    results[key].append(items[2])\n",
    "        else:\n",
    "            items = event.split('__')\n",
    "            for item in items:\n",
    "                it = item.split('_')\n",
    "                for key in results.keys():\n",
    "                    if it[0]==key: results[key].append(it[1])\n",
    "    # check what we are wanting to return \n",
    "    if outcome is None:\n",
    "        return len(results[level])\n",
    "    \n",
    "    # if 'outcome' is assigned a value - identify actual, forecast or sentiment result\n",
    "    if base_event==quote_event=='none': out = 0\n",
    "    else:\n",
    "        res_dict = {}\n",
    "        for key in results.keys():\n",
    "            actual = []\n",
    "            if len(results[key])==0: pass\n",
    "            else:\n",
    "                for res in results[key]:\n",
    "                    # 'better' - actual, 'right' - forecast, 'positive' - sentiment\n",
    "                    if res in ['better','right','positive']: actual.append(1)\n",
    "                    elif res in ['worse','wrong','negative']: actual.append(-1)\n",
    "            res_dict[key] = actual\n",
    "        actual_res = 3*sum(res_dict['high']) + 2*sum(res_dict['medium']) + sum(res_dict['low'])\n",
    "        if actual_res>0 and outcome=='positive': out = 1\n",
    "        elif actual_res<0 and outcome=='negative': out = 1\n",
    "        elif actual_res==0 and outcome=='neutral': out = 1\n",
    "        else: out = 0\n",
    "    return out\n",
    "\n",
    "\n",
    "def feature_engineering(df, filter_out=True, corr_co=0.90):\n",
    "    \n",
    "    # drop any null values\n",
    "    df = df.copy().dropna()\n",
    "    \n",
    "    # create a list of features to drop after feature engineering\n",
    "    to_drop = []\n",
    "    for feature in df.columns:\n",
    "        if df[feature].dtype == 'O' and ('__base_' in feature or '__quote_' in feature):\n",
    "            to_drop.append(feature)\n",
    "    \n",
    "    # this will include all feature engineering to the above OBJECT categorical features\n",
    "    # assuming NA values are taken care of\n",
    "    df['ind__base_mins2next'] = df.ind__base_mins_to_next.apply(lambda x: 2088 if x=='none' else float(x))\n",
    "    df['ind__quote_mins2next'] = df.ind__quote_mins_to_next.apply(lambda x: 2088 if x=='none' else float(x))\n",
    "    \n",
    "    # create binary indicator for whether or not a base/quote currency has the next event coming up (not including 2088 values)\n",
    "    df['ind__base_nextevent'] = df.apply(lambda x: 1 if (x['ind__base_mins2next'] >= x['ind__quote_mins2next'] and\\\n",
    "                                                     x['ind__base_mins2next']!=2088) else 0, axis=1)\n",
    "    df['ind__quote_nextevent'] = df.apply(lambda x: 1 if (x['ind__quote_mins2next'] >= x['ind__base_mins2next'] and\\\n",
    "                                                     x['ind__quote_mins2next']!=2088) else 0, axis=1)\n",
    "    # past event impacts    \n",
    "    df['ind__pastimp_high'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                             level='high'), axis=1)\n",
    "    df['ind__pastimp_med'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                             level='medium'), axis=1)\n",
    "    df['ind__pastimp_low'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                             level='low'), axis=1)\n",
    "    df['ind__past_actual_pos'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__past_actual_neg'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__past_actual_neut'] = df.apply(lambda x: get_event_info(x['ind__base_pastimpact'], x['ind__quote_pastimpact'], \\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    # forecast of event impacts    \n",
    "    df['ind__nextimp_high'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                             level='high'), axis=1)\n",
    "    df['ind__nextimp_med'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                             level='medium'), axis=1)\n",
    "    df['ind__nextimp_low'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                             level='low'), axis=1)\n",
    "    df['ind__next_fore_pos'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__next_fore_neg'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__next_fore_neut'] = df.apply(lambda x: get_event_info(x['ind__base_foreimpact'], x['ind__quote_foreimpact'], \\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    # set FF Alert features for next events\n",
    "    df['ind__ffalert_high']=df.apply(lambda x:get_event_info(x['ind__base_ffalert'],x['ind__quote_ffalert'],level='high'), axis=1)\n",
    "    df['ind__ffalert_med']=df.apply(lambda x:get_event_info(x['ind__base_ffalert'],x['ind__quote_ffalert'],level='medium'), axis=1)\n",
    "    df['ind__ffalert_low']=df.apply(lambda x:get_event_info(x['ind__base_ffalert'],x['ind__quote_ffalert'],level='low'), axis=1)\n",
    "    # set sentiment analysis features\n",
    "    df['ind__pastimp_sent_pos'] = df.apply(lambda x:get_event_info(x['ind__base_pastevent_sent'],x['ind__quote_pastevent_sent'],\\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__pastimp_sent_neg'] = df.apply(lambda x:get_event_info(x['ind__base_pastevent_sent'],x['ind__quote_pastevent_sent'],\\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__pastimp_sent_neut'] = df.apply(lambda x:get_event_info(x['ind__base_pastevent_sent'],x['ind__quote_pastevent_sent'],\\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    df['ind__nextimp_sent_pos'] = df.apply(lambda x:get_event_info(x['ind__base_nextevent_sent'],x['ind__quote_nextevent_sent'],\\\n",
    "                                                                outcome='positive'), axis=1)\n",
    "    df['ind__nextimp_sent_neg'] = df.apply(lambda x:get_event_info(x['ind__base_nextevent_sent'],x['ind__quote_nextevent_sent'],\\\n",
    "                                                                outcome='negative'), axis=1)\n",
    "    df['ind__nextimp_sent_neut'] = df.apply(lambda x:get_event_info(x['ind__base_nextevent_sent'],x['ind__quote_nextevent_sent'],\\\n",
    "                                                                outcome='neutral'), axis=1)\n",
    "    # now we should drop all features in 'to_drop' that were used to create these features\n",
    "    df = df.drop(to_drop, axis=1)\n",
    "    \n",
    "    # we are going to make Low == to lowest price if not already\n",
    "    # same for High\n",
    "    df.Low = df.apply(lambda x: x['Low'] if x['Low'] == min([x['Low'],x['High'],x['Open'],x['Close']]) else \\\n",
    "                         min([x['Low'],x['High'],x['Open'],x['Close']]), axis=1)\n",
    "    df.High = df.apply(lambda x: x['High'] if x['High'] == max([x['Low'],x['High'],x['Open'],x['Close']]) else \\\n",
    "                         max([x['Low'],x['High'],x['Open'],x['Close']]), axis=1)\n",
    "    \n",
    "    # set negative values to 0 where wick height was taken\n",
    "    df.wick_high = df.wick_high.apply(lambda x: 0 if x<0 else x)\n",
    "    df.wick_low = df.wick_low.apply(lambda x: 0 if x<0 else x)\n",
    "    \n",
    "    # convert features to scale over their exchange rates to normalize the rows\n",
    "    # best way to deal with wicks -- we will try wick / range pct\n",
    "    df['wick_low_pct'] = df.wick_low / (df.High - df.Low)\n",
    "    df['wick_high_pct'] = df.wick_high / (df.High - df.Low)\n",
    "    df['ind__midbb_pct'] = 10000*(df.Open - df.ind__mid_bb) / df.Open\n",
    "    df['ind__bbdif_pct'] = 10000*df.ind__bb_dif / df.ind__low_bb \n",
    "    df['spread_pct'] = 10000*df.spread/df.Close\n",
    "    #df['biddif_pct'] = df.bid_diff/df.spread #  -- only need one, the other is infered -- otherwise singularity potential\n",
    "    df['askdif_pct'] = df.ask_diff/df.spread\n",
    "    df['ind__bbslope_pct'] = 10000*df.ind__midbb_slope / df.Close\n",
    "    # np.polyfit calculates the residuals as sum of squared errors -- therefore divide by Close**2 to normalize\n",
    "    df['ind__trendresids_pct'] = 10000*df.ind__trend_residuals / df.Close**2\n",
    "    \n",
    "    # define the target variable\n",
    "    df = df[df.exch_rate_dif!=0]\n",
    "    df['exratedif_pct'] = 10000*df.exch_rate_dif/df.Close\n",
    "    df['dir__up_1__down_0'] = df.direction.apply(lambda val: 1 if val=='up' else 0)\n",
    "    df.drop(['exch_rate_dif','direction'],axis=1,inplace=True)\n",
    "    \n",
    "    # drop remaining features that seperate symbols by rate\n",
    "    drop_other = ['Close_Low_avg','Close_High_avg','ind__up_bb','ind__low_bb','expiration','Close','Open','Low','High',\n",
    "                  'wick_high','wick_low','ind__bb_dif','spread','bid_diff','ask_diff','ind__midbb_slope','ind__mid_bb',\n",
    "                  'ind__trend_residuals']\n",
    "    df = df.drop(drop_other, axis=1)\n",
    "    \n",
    "    # we will create dummy variable for the symbol features, and add on remaining symbols\n",
    "    ## WE ARE GOING TO SUPPRESS THESE VALUES, MODEL PERFORMS BETTER WITHOUT THEM\n",
    "#     symbol_dummies = pd.get_dummies(df.symbol, drop_first=True)\n",
    "#     df = pd.concat([df,symbol_dummies], axis=1)\n",
    "#     df = df.drop(['symbol'], axis=1)\n",
    "\n",
    "    # filter out where we did not pull currency strengths\n",
    "    df = df[df.ind__base_strength!=0]\n",
    "    \n",
    "    # filter out features that are a constant value (where only 1 or 0 was recorded in data)\n",
    "    to_drop=[]\n",
    "    for col in df.describe().columns:\n",
    "        desc = df.describe()[col]\n",
    "        if desc['mean']==0 and desc['max']-desc['min']==0:\n",
    "            to_drop.append(col)\n",
    "    if len(to_drop)>0:\n",
    "        print_bold('DROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:')\n",
    "        print(to_drop)\n",
    "        df = df.drop(to_drop, axis=1)\n",
    "        \n",
    "    # create a list of features based on correlation priority\n",
    "    corr = df.drop(['exratedif_pct'],axis=1).corr()['dir__up_1__down_0']\n",
    "    corr_priority = abs(corr).sort_values(ascending=False)[1:]\n",
    "    # filter out features that are too correlated\n",
    "    corr_drop=[]\n",
    "    if filter_out:\n",
    "        corrs = df.drop(['exratedif_pct','dir__up_1__down_0'],axis=1).corr()\n",
    "        for row in corrs.index:\n",
    "            for col in corrs.columns:\n",
    "                if row == col: break\n",
    "                else:\n",
    "                    if abs(corrs.loc[row,col]) > corr_co:\n",
    "                        # check which feature has a higher correlation to the target feature and drop the smaller\n",
    "                        if corr_priority[col] >= corr_priority[row]: corr_drop.append(row)\n",
    "                        else: corr_drop.append(col)\n",
    "        corr_drop = np.unique(corr_drop).tolist()\n",
    "        if len(corr_drop)>0:\n",
    "            print_bold('\\nDROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:')\n",
    "            print(corr_drop)\n",
    "            df = df.drop(corr_drop, axis=1)\n",
    "    # check if we need to drop symbol or not\n",
    "    if 'symbol' in df.columns: df.drop(['symbol'], axis=1, inplace=True)\n",
    "        \n",
    "    df = df.dropna()    \n",
    "    print_bold('\\nSHAPE OF DF:')\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 dataframes based on the correlation coefficients specified previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__rsi_rolling', 'ind__stoK_rolling']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(35213, 62)\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "df1 = feature_engineering(df.copy(), corr_co=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__midbb_pct', 'ind__rsi_rolling', 'ind__stoK_rolling', 'ind__stochK', 'ind__surf_pct', 'move_up']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(35213, 58)\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "df2 = feature_engineering(df.copy(), corr_co=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDROPPING THE FOLLOWING FEATURES DUE TO LACK OF FEATURE VARIANCE:\u001b[0m\n",
      "['ind__ffalert_high', 'ind__ffalert_med']\n",
      "\u001b[1m\n",
      "DROPPING THE FOLLOWING FEATURES DUE TO HIGH CORRELATION:\u001b[0m\n",
      "['ind__base_numevents', 'ind__base_recentevent', 'ind__base_strength', 'ind__bbdif_pct', 'ind__bbslope_pct', 'ind__midbb_pct', 'ind__past_actual_neut', 'ind__quote_mins2next', 'ind__quote_recentevent', 'ind__quote_strength', 'ind__rsi_rolling', 'ind__stoK_rolling', 'ind__stochK', 'ind__surf_pct', 'move_up']\n",
      "\u001b[1m\n",
      "SHAPE OF DF:\u001b[0m\n",
      "(35213, 49)\n"
     ]
    }
   ],
   "source": [
    "df3 = feature_engineering_engineering(df.copy(), corr_co=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import estimators/algorithms to be tested for our models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# import modules for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, accuracy_score\n",
    "\n",
    "# import modules for cross validation\n",
    "from sklearn.model_selection import cross_validate, KFold  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_cls(df, get_model=False, multi=False):\n",
    "     \n",
    "    # split data --\n",
    "    X = df.drop(['exratedif_pct','dir__up_1__down_0'], axis=1)\n",
    "    y = df.dir__up_1__down_0\n",
    "    X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    # train and cross validate all models\n",
    "    cv_results, models_tbl, cv_method_tbl = [],[],[]\n",
    "    # set random state\n",
    "    rs = 1\n",
    "    # Create a list of classifiers to be tested\n",
    "    classifiers = []\n",
    "    classifiers.append(AdaBoostClassifier(RandomForestClassifier(n_estimators=100),n_estimators=100, random_state=rs))\n",
    "    classifiers.append(RandomForestClassifier(n_estimators=100, random_state=rs))\n",
    "\n",
    "    # cv method\n",
    "    cv_list = []\n",
    "    cv_list.append(KFold(n_splits=10))\n",
    "\n",
    "    # set variable for model count\n",
    "    i = 1\n",
    "    for cv_method in cv_list:\n",
    "        for cls in classifiers:\n",
    "            # perform cross validation\n",
    "            if multi: avg = 'macro'\n",
    "            else: avg='binary'\n",
    "            scoring = {'accuracy':make_scorer(accuracy_score) ,\n",
    "                       'precision':make_scorer(precision_score, average=avg),\n",
    "                       'f1':make_scorer(f1_score, average=avg)}\n",
    "            scores = cross_validate(cls, X=X_train, y=y_train, cv=cv_method, scoring=scoring, )#, n_jobs=-1)\n",
    "            # assign variable for naming classifiers\n",
    "            cls_ = str(cls).split('(')[0]\n",
    "            models_tbl.append(cls_+' - cvm_'+''.join([c for c in str(str(cv_method).split('(')[0]) if c.isupper()]))\n",
    "            cv_method_tbl.append(str(cv_method).split('(')[0])\n",
    "            cv_results.append(scores)\n",
    "            # next model\n",
    "            i+=1 \n",
    "    ## ---------------------------\n",
    "    # Create table to show results\n",
    "    fit_time, score_time = [],[]\n",
    "    accuracy, precision, f1 = [],[],[]\n",
    "\n",
    "    print_bold('Performing Model Analysis...\\n')\n",
    "    for n, item in enumerate(cv_results):\n",
    "        for key in item:\n",
    "            if key == 'fit_time': fit_time.append(item[key].mean())\n",
    "            elif key == 'score_time': score_time.append(item[key].mean())\n",
    "            elif 'accuracy' in key: accuracy.append(np.mean(item[key]))\n",
    "            elif 'precision' in key: precision.append(np.mean(item[key]))\n",
    "            elif 'f1' in key: f1.append(np.mean(item[key]))\n",
    "        \n",
    "    result_tbl = pd.DataFrame(data={'CV Method':cv_method_tbl, 'accuracy':accuracy, 'precision':precision, 'f1':f1,\n",
    "                                    'Fit time':fit_time, 'Score time':score_time},\n",
    "                              index=models_tbl)\n",
    "    print('Model Results...')\n",
    "    display(result_tbl.sort_values(by='accuracy', ascending=False))\n",
    "    \n",
    "    if get_model:\n",
    "        pass\n",
    "\n",
    "def class_param_tuning(df, folds=5, estimator='adb'):\n",
    "    '''\n",
    "    Types of estimators: 'adb', 'svc'\n",
    "    '''\n",
    "    if estimator=='adb':\n",
    "        est = AdaBoostClassifier(RandomForestClassifier(random_state=1), random_state=1)\n",
    "        params = {'base_estimator__n_estimators':[200,300],\n",
    "                  'n_estimators':[300,400,500],\n",
    "                  'learning_rate':[1,0.1,0.0001]\n",
    "             }\n",
    "    elif estimator=='svc':\n",
    "        est = SVC()\n",
    "        params = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "                  'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                  'degree':[2,3,5],\n",
    "                  'kernel': ['rbf','poly','sigmoid']}  \n",
    "\n",
    "    X = df.drop(['exratedif_pct','dir__up_1__down_0'], axis=1)\n",
    "    y = df.dir__up_1__down_0\n",
    "    X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # save X columns as variable feat_single\n",
    "    feats = X.columns\n",
    "    \n",
    "    # cv method\n",
    "    kfold = KFold(folds)\n",
    "\n",
    "    #gscv = GridSearchCV(est, param_grid=params, scoring='accuracy', cv=kfold, verbose=3) #, n_jobs=-1)\n",
    "    gscv = RandomizedSearchCV(est, param_distributions=params, verbose=3, n_iter=10, scoring=['accuracy'],\n",
    "                              cv=kfold, refit='accuracy', n_jobs=-1)\n",
    "    gscv.fit(X_train, y_train)\n",
    "    print(f'\\nBest score: {gscv.best_score_}')\n",
    "    print(f'Best parameters: {gscv.best_params_}')\n",
    "\n",
    "    # create model from best parameters\n",
    "    model = est.set_params(**gscv.best_params_)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "#     df_disp = pd.DataFrame({'features':X.columns ,'importance':model.feature_importances_})\\\n",
    "#     .sort_values(by='importance', ascending=False).head(10)\n",
    "#     display(df_disp)\n",
    "    return model, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a quick model evaluation to see which classifier is best fits our dataframes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPerforming Model Analysis...\n",
      "\u001b[0m\n",
      "Model Results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>Fit time</th>\n",
       "      <th>Score time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.593899</td>\n",
       "      <td>0.604666</td>\n",
       "      <td>0.579486</td>\n",
       "      <td>5.262919</td>\n",
       "      <td>0.182749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.591708</td>\n",
       "      <td>0.601352</td>\n",
       "      <td>0.578913</td>\n",
       "      <td>4.847463</td>\n",
       "      <td>0.171971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                CV Method  accuracy  precision        f1  \\\n",
       "AdaBoostClassifier - cvm_KF         KFold  0.593899   0.604666  0.579486   \n",
       "RandomForestClassifier - cvm_KF     KFold  0.591708   0.601352  0.578913   \n",
       "\n",
       "                                 Fit time  Score time  \n",
       "AdaBoostClassifier - cvm_KF      5.262919    0.182749  \n",
       "RandomForestClassifier - cvm_KF  4.847463    0.171971  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df with correlation coefficient at 0.9\n",
    "sklearn_cls(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPerforming Model Analysis...\n",
      "\u001b[0m\n",
      "Model Results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>Fit time</th>\n",
       "      <th>Score time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.597712</td>\n",
       "      <td>0.607326</td>\n",
       "      <td>0.586046</td>\n",
       "      <td>4.508573</td>\n",
       "      <td>0.172843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.596697</td>\n",
       "      <td>0.606661</td>\n",
       "      <td>0.584116</td>\n",
       "      <td>5.003068</td>\n",
       "      <td>0.180203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                CV Method  accuracy  precision        f1  \\\n",
       "RandomForestClassifier - cvm_KF     KFold  0.597712   0.607326  0.586046   \n",
       "AdaBoostClassifier - cvm_KF         KFold  0.596697   0.606661  0.584116   \n",
       "\n",
       "                                 Fit time  Score time  \n",
       "RandomForestClassifier - cvm_KF  4.508573    0.172843  \n",
       "AdaBoostClassifier - cvm_KF      5.003068    0.180203  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df with correlation coefficient at 0.75\n",
    "sklearn_cls(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPerforming Model Analysis...\n",
      "\u001b[0m\n",
      "Model Results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>Fit time</th>\n",
       "      <th>Score time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.586190</td>\n",
       "      <td>0.595452</td>\n",
       "      <td>0.573087</td>\n",
       "      <td>3.757510</td>\n",
       "      <td>0.180488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier - cvm_KF</th>\n",
       "      <td>KFold</td>\n",
       "      <td>0.584121</td>\n",
       "      <td>0.593658</td>\n",
       "      <td>0.570160</td>\n",
       "      <td>4.217791</td>\n",
       "      <td>0.183378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                CV Method  accuracy  precision        f1  \\\n",
       "RandomForestClassifier - cvm_KF     KFold  0.586190   0.595452  0.573087   \n",
       "AdaBoostClassifier - cvm_KF         KFold  0.584121   0.593658  0.570160   \n",
       "\n",
       "                                 Fit time  Score time  \n",
       "RandomForestClassifier - cvm_KF  3.757510    0.180488  \n",
       "AdaBoostClassifier - cvm_KF      4.217791    0.183378  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df with correlation coefficient at 0.6\n",
    "sklearn_cls(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial model evaluation brief:**\n",
    "- Our model results are very similar for all of our dataframes. The major difference is in efficiency of fitting the model.\n",
    "\n",
    "**Next steps**\n",
    "- We will perform hyperparameter tuning for the model created using the second dataframe which used a correlation coefficient of 0.75 to filter features.\n",
    "- For the hyperparameter tuning, we will create an ensembled model with AdaBoostClassifier and our top RandomForestClassifier. The parameter grid can be seen in the function named \"class_param_tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  50 | elapsed:  1.5min remaining:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.5990101018296888\n",
      "Best parameters: {'n_estimators': 400, 'learning_rate': 0.0001, 'base_estimator__n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.65      0.62      5239\n",
      "           1       0.63      0.58      0.60      5325\n",
      "\n",
      "    accuracy                           0.61     10564\n",
      "   macro avg       0.61      0.61      0.61     10564\n",
      "weighted avg       0.61      0.61      0.61     10564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m1,f1 = class_param_tuning(df2, folds=5, estimator='adb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the model into the models folder using pickle for future predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# saving model file\n",
    "file_name = 'models/model_1.pkl'\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(m1, file)\n",
    "    \n",
    "# write the lists of features to a file\n",
    "with open('models/model_1_features.txt', 'w') as filehandle:\n",
    "    for listitem in f1:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion, Recommendations, and Future Considerations\n",
    "- Starting from scratch, I was able to create a database which was then used to create a machine learning model to help identify trading opportunities. The model was trained on features that were created which are typically used for trading strategies for technical and fundamental analysis. We then monitored the price action after a period to determine if price went up or down, and then created a classification model off of this data.\n",
    "- The final model performed better than expecting, yielding an accuracy of roughly 61% predictability. `The model with the best performance was an ensamble of the AdaBoostClassifier with RandomForestClassifier, tuned using the RandomSearchCV method.`\n",
    "- I have used this model to try and trade on a demo account, and I have not had the success that I want.\n",
    "\n",
    "**For future recommendations** I will be trying out a few different strategies. First, I would like to try and refine my dataframe more by looking at different time periods. I've noticed that the 1 minute chart is not as real-time as I would have liked. Due to this, it lead to poor data for my model and trade execution. The longer the time period, the more predictive our model will be, so I will consider looking at a 15 minute candle chart instead. Another consideration I have for the future would be to look at a multi-class model, where the multi-classification would come from identifying trades that have a much larger price difference over the specified period, hoping that the features would be able to identify more correlations to these larger moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
