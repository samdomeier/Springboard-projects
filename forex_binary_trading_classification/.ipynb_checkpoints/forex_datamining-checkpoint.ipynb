{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamining Notebook\n",
    "This notebook consists of functions that are designed to pull specific technical and sentimental information regarding a foreign exhange currency pair of your choice.\n",
    "\n",
    "### The Information\n",
    "All of the information gathered in this notebook is stored locally to a CSV file where the data is then used to create a model to predict binary options using a classification approach.\n",
    "\n",
    "The notebook is divided into 5 sections listed in the [Table of Contents](#Table-of-Contents:).\n",
    "- The first section is where the initial dataframe is collected, it includes all of the information provided by the Alpha Vantage API, which is used to collect the currency pair exchange rate from the past day, by the minute.\n",
    "- The function used is relatively complex, since it corrects issues that I found in the API to ensure that the data I am collecting is relative to the time that is shown. This is vital for the correlation of the other functions that are used to collect timely infomration like news sources, tweets, and currency strength at the time.\n",
    "- The other sections are relatively self-explanatory like web scrapping and twitter sentiment analysis.\n",
    "- The section labeled \"DATAMINING FUNCTION\" is where all of the functions come together. This is where all of the information is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import modf\n",
    "import glob\n",
    "import sys\n",
    "from calendar import month_abbr\n",
    "\n",
    "## documentation for Alpha Vantage : https://www.alphavantage.co/documentation/\n",
    "from alpha_vantage.foreignexchange import ForeignExchange\n",
    "\n",
    "# trendline analysis modules\n",
    "from trendln import calc_support_resistance\n",
    "\n",
    "# web scraping, twitter handling, and word processing modules\n",
    "import bs4\n",
    "import requests\n",
    "import cloudscraper\n",
    "from collections import Counter as count\n",
    "import operator\n",
    "import tweepy as tw\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# alphavantage api key\n",
    "API_KEY = 'your key here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "- [DATAFRAME COLLECTION](#DATAFRAME-COLLECTION)\n",
    "- [WEB SCRAPING](#WEB-SCRAPING)\n",
    "- [TWITTER SENTIMENT ANALYSIS](#TWITTER-SENTIMENT-ANALYSIS)\n",
    "- [DATAMINING FUNCTION](#DATAMINING-FUNCTION)\n",
    "- [EXECUTE DATAMINING](#Execute-Datamining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAME COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicators will be prefixed with 'ind__' - i.e. 'ind__rsi'\n",
    "\n",
    "def intraday_data(symbol, data=None, interval='1min', key=API_KEY, first=True, restart=False):\n",
    "    \n",
    "    global time_dict\n",
    "    global new\n",
    "    fe = ForeignExchange(key=key, output_format='pandas')\n",
    "    \n",
    "    def call_api():\n",
    "        print('Calling API for rates.')\n",
    "        base = symbol[:3]\n",
    "        quote = symbol[3:]\n",
    "        # grab current 5 decimal exchange rate\n",
    "        d,_ = fe.get_currency_exchange_rate(base, quote)\n",
    "        data,_ = fe.get_currency_exchange_intraday(base, quote, interval=interval, outputsize='full')        \n",
    "        data.columns = ['Open','High','Low','Close']\n",
    "        data = data.sort_index(ascending=True)\n",
    "        for key in d.keys():\n",
    "            if 'exchange rate' in key.lower():\n",
    "                cur = np.float64(d[key])\n",
    "            elif 'bid price' in key.lower():\n",
    "                bid = np.float64(d[key])\n",
    "            elif 'ask price' in key.lower():\n",
    "                ask = np.float64(d[key])\n",
    "            elif 'refreshed' in key.lower():\n",
    "                update = datetime.datetime.strptime(d[key][0], '%Y-%m-%d %H:%M:%S')\n",
    "        print(datetime.datetime.now())\n",
    "        print(data['Close'].index[-1],data['Close'][-1])\n",
    "        print(d['6. Last Refreshed'][0],d['5. Exchange Rate'][0])\n",
    "        return data, cur, bid, ask, update\n",
    "    \n",
    "    if first:\n",
    "        # gather time date when begining\n",
    "        initial = datetime.datetime.now()\n",
    "        print('Creating initial df:',initial)\n",
    "        time_dict = {'last':initial, 'second':initial.second}\n",
    "        # start api search\n",
    "        data, cur, bid, ask, update = call_api()\n",
    "        # check if the exch rate time is a minute greater than the last data entry in the table\n",
    "        if update.minute-1 == data[-1:].index[0].minute and update.second <= 10:\n",
    "            # set the most recent close value to the 5 decimal value\n",
    "            data.loc[data[-1:].index[0], 'Close'] = cur\n",
    "            time_dict['next_index'] = update\n",
    "            # create an initail pandas Timestamp object to match datatypes for indexing the table\n",
    "            time_dict['initial_index'] = data[-1:].index[0]\n",
    "            print('initial index:', data[-1:].index[0])\n",
    "        elif update.minute == initial.minute:\n",
    "            print('Intraday data is off one step; setting initial entry manually.')\n",
    "            data = data[:-1]\n",
    "            # set the most recent close value to the 5 decimal value\n",
    "            data.loc[data[-1:].index[0], 'Close'] = cur\n",
    "            time_dict['next_index'] = update\n",
    "            # create an initail pandas Timestamp object to match datatypes for indexing the table\n",
    "            time_dict['initial_index'] = data[-1:].index[0]\n",
    "            print('initial index:', data[-1:].index[0])\n",
    "        else:\n",
    "            print('First time entry not valid, restart program.')\n",
    "            print(update.strftime('%Y-%m-%d %H:%M'),'current ER -- vs -- last row',data[-1:].index[0])\n",
    "            raise\n",
    "    elif restart: # This will execute if the data 'restarted' -- had to break due to lag in API\n",
    "        # start api search\n",
    "        while True:\n",
    "            new, cur, bid, ask, update = call_api()\n",
    "            # check if the exch rate time is a minute greater than the last data entry in the table\n",
    "            if update.minute-1 == new[-1:].index[0].minute and update.second <= 10:\n",
    "                time_dict['last'] = time_dict['last'] + datetime.timedelta(0,60)\n",
    "                # setting all previously saved data to the new dataframe -- it will only go until it left off\n",
    "                new.loc[data.index] = data\n",
    "                data = new.copy()\n",
    "                # set the most recent close value to the 5 decimal value\n",
    "                data.loc[data[-1:].index[0], 'Close'] = cur\n",
    "                time_dict['next_index'] = update\n",
    "            else:\n",
    "                print('Need to restart program in full.')\n",
    "                raise\n",
    "        \n",
    "    else: # this will be the most common aspect to the statement\n",
    "        print('Next data entry being created. Currently in while loop waiting 1 minute.')\n",
    "        while True:\n",
    "            now = datetime.datetime.now()\n",
    "            # this statement is checking if it has been more than a minute, on the same second (or more), and less than\n",
    "            # the next update time.\n",
    "            time_diff = now - time_dict['last']\n",
    "            if modf(time_diff.seconds/60)[1] == 1 and now.second >= time_dict['second'] and \\\n",
    "            time_diff.seconds < 70:\n",
    "                time_dict['last'] = time_dict['last'] + datetime.timedelta(0,60)\n",
    "                try: new, cur, bid, ask, update = call_api() # call the api\n",
    "                except:\n",
    "                    try:\n",
    "                        print('Calling API one more time.')\n",
    "                        time.sleep(1)\n",
    "                        # start api search\n",
    "                        new, cur, bid, ask, update = call_api()\n",
    "                    except:\n",
    "                        print('No API call to Alphavantage. Stopping program.')\n",
    "                        raise # we can just wait until next row value                \n",
    "                restart = False\n",
    "                break\n",
    "            elif time_diff.seconds >= 70:\n",
    "                # we will skip this entry and wait a minute or two until we call the api again\n",
    "                # we need to replace the data we just saved with the next updated dataframe\n",
    "                print('Restarting because of large time difference between entries:',time_diff,'s')\n",
    "                initial_id = time_dict['initial_index']\n",
    "                data = data.loc[initial_id:]\n",
    "                restart = True\n",
    "                return data, restart\n",
    "                \n",
    "            time.sleep(0.5) # last statement in while loop\n",
    "        \n",
    "        #display(new.tail())\n",
    "        # check if the exch rate time is a minute greater than the last data entry in the table\n",
    "        if (update.minute-1 == new[-1:].index[0].minute or update.minute+59 == new[-1:].index[0].minute) and update.second <= 10:\n",
    "            # set the most recent close value to the 5 decimal value\n",
    "            new_row = new[-1:].copy()\n",
    "            time_dict['next_index'] = update\n",
    "        # check the current update time, if it is a minute greater than the other, and no more than 10 s >\n",
    "        elif (update.minute-1 == time_dict['next_index'].minute or update.minute+59 == time_dict['next_index'].minute) \\\n",
    "        and update.second-time_dict['next_index'].second <= 10:\n",
    "            # trim the last row indexed because it is a false index\n",
    "            new = new[:-1]\n",
    "            new_row = new[-1:].copy()\n",
    "            oldtime = data[-1:].index[0]\n",
    "            # get next time value\n",
    "            new_index = oldtime + datetime.timedelta(0,60)\n",
    "            new_row['date'] = new_index\n",
    "            new_row.set_index('date',inplace=True)\n",
    "            time_dict['next_index'] = update\n",
    "        else:\n",
    "            print('Restarting because of lag in API.')\n",
    "            initial_id = time_dict['initial_index']\n",
    "            data = data.loc[initial_id:]\n",
    "            restart = True\n",
    "            return data, restart\n",
    "        print('next time index:', update.strftime('%Y-%m-%d %H:%M'))\n",
    "        new_row.Open[0] = data[-1:].Close[0]\n",
    "        new_row.Close[0] = cur\n",
    "        ind_initial = time_dict['initial_index']\n",
    "        data = data.loc[ind_initial:][['Open','High','Low','Close']]\n",
    "        new.loc[data.index] = data\n",
    "        data = pd.concat([new[:-1], new_row])\n",
    "    \n",
    "    data = data[['Open','High','Low','Close']]\n",
    "    # add the symbol to the dataframe\n",
    "    data['symbol'] = symbol\n",
    "    # these columns will be used to pull support and resistance trendlines from\n",
    "    data['Close_Low_avg'] = (data.Close + data.Low) / 2\n",
    "    data['Close_High_avg'] = (data.Close + data.High) / 2\n",
    "    # we will use these columns as ML features for the machine\n",
    "    data['move_up'] = (data['Close'] - data['Open']).apply(lambda x: 1 if x>0 else 0)\n",
    "    data['wick_high'] = data.apply(lambda x: x.High - max(x.Close, x.Open), axis=1)\n",
    "    data['wick_low'] = data.apply(lambda x: min(x.Close, x.Open) - x.Low, axis=1)\n",
    "    restart = False\n",
    "#     display(data.tail())\n",
    "#     print('shape',data.shape)\n",
    "    return (data, bid, ask), restart\n",
    "\n",
    "\n",
    "def stochastic(data, period=5, smoothK=2, smoothD=1):\n",
    "\n",
    "    lo = data.Low\n",
    "    hi = data.High\n",
    "    cl = data.Close\n",
    "    \n",
    "    stoch = 100 * (cl - cl.rolling(period).min()) / (cl.rolling(period).max() - cl.rolling(period).min())\n",
    "    stoch_K = round(stoch.rolling(smoothK).mean(), 3)\n",
    "    stoch_D = round(stoch_K.rolling(smoothD).mean(), 3)\n",
    "    stoch_K.name = 'ind__stochK'\n",
    "    stoch_D.name = 'ind__stochD'\n",
    "    \n",
    "    return stoch_K, stoch_D\n",
    "    \n",
    "def RSI(data, period=5):\n",
    "    \n",
    "    series = data.Close\n",
    "    \n",
    "    delta = series.diff().dropna()\n",
    "    ups = delta * 0\n",
    "    downs = ups.copy()\n",
    "    ups[delta > 0] = delta[delta > 0]\n",
    "    downs[delta < 0] = -delta[delta < 0]\n",
    "    ups[ups.index[period-1]] = np.mean( ups[:period] ) #first value is sum of avg gains\n",
    "    ups = ups.drop(ups.index[:(period-1)])\n",
    "    downs[downs.index[period-1]] = np.mean( downs[:period] ) #first value is sum of avg losses\n",
    "    downs = downs.drop(downs.index[:(period-1)])\n",
    "    rs = ups.ewm(com=period-1,min_periods=0,adjust=False,ignore_na=False).mean() / \\\n",
    "         downs.ewm(com=period-1,min_periods=0,adjust=False,ignore_na=False).mean() \n",
    "    rsi = round(100 - 100 / (1 + rs), 3)\n",
    "    rsi.name = 'ind__rsi'\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def bollinger_bands(data, period=20, std=2, source='Close', dif_period=1):\n",
    "    \n",
    "    series = data[source]\n",
    "    \n",
    "    middle = round(series.rolling(period).mean(), 6)\n",
    "    upper = round(middle + (series.rolling(period).std() * std), 6)\n",
    "    lower = round(middle - (series.rolling(period).std() * std), 6)\n",
    "    boll = pd.concat([middle, upper, lower], axis=1)\n",
    "    boll.columns = ['ind__mid_bb','ind__up_bb','ind__low_bb']\n",
    "    boll['ind__bb_dif'] = boll.ind__up_bb - boll.ind__low_bb\n",
    "    boll['ind__bb_dif'] = boll.ind__bb_dif.rolling(dif_period).mean()\n",
    "    \n",
    "    def volatile(val, data):\n",
    "        if val >= data.ind__bb_dif.quantile(0.75): return 1\n",
    "        else: return 0\n",
    "        \n",
    "    boll['ind__volatile'] = boll.ind__bb_dif.apply(lambda x: volatile(x, boll))\n",
    "    return boll\n",
    "\n",
    "\n",
    "def bb_analysis(data, source='Close', window=3, sensitivity=0.85, surf_count=2):\n",
    "    global up\n",
    "    global cur\n",
    "    \n",
    "    # create series for upper and lower boundaries\n",
    "    up = data['ind__up_bb']\n",
    "    low = data['ind__low_bb']\n",
    "    mid = data['ind__mid_bb']\n",
    "    sour = data[source]\n",
    "    # create indicators for relation to upper and lower bb\n",
    "    data['ind__outside_up'] = (up - sour).apply(lambda x: 1 if x<0 else 0)\n",
    "    data['ind__outside_low'] = (sour - low).apply(lambda x: 1 if x<0 else 0)\n",
    "    # compare the rolling average of the current price vs the width of the bollinger bands\n",
    "    data['ind__surf_pct'] = ((sour - low)/(up - low)).rolling(window).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_bestfit(data, period=3):\n",
    "    '''\n",
    "    Going to use this to plot line of best fit to find the middle bollinger band slope at the current point.\\\n",
    "We are going to only apply this to the 'ENRTRY' point, and pull the past x number of points. Do this for short 3, and a long 50\\\n",
    "do this for CLOSING on long (will be used for current trending) and MID_BB on short.\n",
    "    '''\n",
    "    ser = data[-period:]\n",
    "    xpts = range(0,period)\n",
    "    ypts = [y for y in ser]\n",
    "    vals, resid, _,_,_ = np.polyfit(xpts, ypts, deg=1, full=True)\n",
    "    return vals, resid\n",
    "\n",
    "\n",
    "def ind_acceleration(data_indicator, look_back=1):\n",
    "    return data_indicator.diff(look_back)\n",
    "    \n",
    "\n",
    "def get_rolling(data_indicator, window=2):\n",
    "    return data_indicator.rolling(window).mean()\n",
    "\n",
    "\n",
    "def mins_maxs(series, limit=15):\n",
    "    mins,maxs = [],[]\n",
    "    lim = limit\n",
    "    # add current exchange rate\n",
    "    for i in np.arange(0,lim,1):\n",
    "        sec = series.iloc[int((i/lim)*len(series)):int(((i+1)/lim)*len(series))]\n",
    "        lastmin = max(series)\n",
    "        lastmax = min(series)\n",
    "        for idx,n in enumerate(sec.index):\n",
    "            if sec.loc[n] < lastmin and n not in series.iloc[-20:].index:\n",
    "                    minval, nmin, minidx = sec.loc[n], n, idx+len(sec)*i\n",
    "                    lastmin = minval\n",
    "            if sec.loc[n] > lastmax and n not in series.iloc[-20:].index:\n",
    "                maxval, nmax, maxidx = sec.loc[n], n, idx+len(sec)*i\n",
    "                lastmax = maxval\n",
    "        try: mins.append((nmin,minval,minidx))\n",
    "        except: pass\n",
    "        try: maxs.append((nmax,maxval,maxidx))\n",
    "        except: pass\n",
    "    # next we will filter out close points and return only the max/min for the area\n",
    "    mns=[]\n",
    "    for i,mn in enumerate(mins):\n",
    "        m=mn[2]\n",
    "        if i==0 or i==len(mins)-1:\n",
    "            if i==0 and mins[i+1][2]-m>10: mns.append(mn)\n",
    "            elif i==0 and m==min([m,mins[i+1][2]]): mns.append(mn)\n",
    "            if i==len(mins)-1 and m-mins[i-1][2]>10: mns.append(mn)\n",
    "            elif i==len(mins)-1 and m==min([m,mins[i-1][2]]): mns.append(mn)\n",
    "        elif mins[i+1][2]-m>10 and m-mins[i-1][2]>10: mns.append(mn)\n",
    "        elif mins[i+1][2]-m>10 and m-mins[i-1][2]<=10 and m==min([m,mins[i-1][2]]): mns.append(mn)\n",
    "        elif mins[i+1][2]-m<=10 and m-mins[i-1][2]>10 and m==min([m,mins[i+1][2]]): mns.append(mn)\n",
    "    mxs=[]\n",
    "    for i,mx in enumerate(maxs):\n",
    "        m=mx[2]\n",
    "        if i==0 or i==len(maxs)-1:\n",
    "            if i==0 and maxs[i+1][2]-m>10: mxs.append(mx)\n",
    "            elif i==0 and m==max([m,maxs[i+1][2]]): mxs.append(mx)\n",
    "            if i==len(maxs)-1 and m-maxs[i-1][2]>10: mxs.append(mx)\n",
    "            elif i==len(maxs)-1 and m==max([m,maxs[i-1][2]]): mxs.append(mx)\n",
    "        elif maxs[i+1][2]-m>10 and m-maxs[i-1][2]>10: mxs.append(mx)\n",
    "        elif maxs[i+1][2]-m>10 and m-maxs[i-1][2]<=10 and m==max([m,maxs[i-1][2]]): mxs.append(mx)\n",
    "        elif maxs[i+1][2]-m<=10 and m-maxs[i-1][2]>10 and m==max([m,maxs[i+1][2]]): mxs.append(mx)\n",
    "    return mns, mxs\n",
    "\n",
    "\n",
    "def get_riemannsum(series, m, b):\n",
    "    rsum=0\n",
    "    for i,val in enumerate(series):\n",
    "        y = i*m+b\n",
    "        dif = y-val\n",
    "        rsum += dif\n",
    "    return rsum / len(series)\n",
    "\n",
    "\n",
    "# add in a filter to grab the values in results where the 'xs' must contain the current exch index (len(series))\n",
    "def get_trendlines(series, mins_maxs, mm_str='max'):\n",
    "    results=[]\n",
    "    cur = series[-1]\n",
    "    cur_trend = series.diff(1).rolling(30).mean()[-1]\n",
    "    for i,n in enumerate(mins_maxs):\n",
    "        for ii in range(len(mins_maxs)):\n",
    "            for iii in range(len(mins_maxs)):\n",
    "                xinds = [n[0], mins_maxs[ii+i+1][0], mins_maxs[iii+ii+i+2][0]]\n",
    "                xs = [n[2], mins_maxs[ii+i+1][2], mins_maxs[iii+ii+i+2][2]]\n",
    "                ys = [n[1], mins_maxs[ii+i+1][1], mins_maxs[iii+ii+i+2][1]]\n",
    "                vals = np.polyfit(xs,ys,deg=1,full=True)\n",
    "                pred = len(series)*vals[0][0]+vals[0][1]\n",
    "                raw = cur-pred\n",
    "                dif = abs(raw)\n",
    "                riemann = get_riemannsum(series, vals[0][0], vals[0][1])\n",
    "                results.append((vals[0],vals[1],xs,ys,xinds,dif,riemann,raw))\n",
    "                if iii+ii+i+2 == len(mins_maxs)-1:\n",
    "                    break\n",
    "            if ii+i+1 == len(mins_maxs)-2:\n",
    "                break\n",
    "        if i == int(len(mins_maxs)*(2/3)) or i==len(mins_maxs)-3: # set a stop at 2/3 of min_maxs so that we capture long trends\n",
    "            break\n",
    "    # keep the top 50% of results based on residuals\n",
    "    rem = pd.Series([res[1] for res in results]).quantile(.50)\n",
    "    if mm_str=='max':\n",
    "        results = [res for res in results if (len(res[1])!=0 and res[1]<rem and res[6]>0)]\n",
    "        # find the top 75th quantile for the riemann sum and only keep trendlines with the better than quantile\n",
    "        quant = pd.Series([r[6] for r in results]).quantile(0.65)\n",
    "        results = [res for res in results if res[6]>=quant]\n",
    "    elif mm_str=='min':\n",
    "        results = [res for res in results if (len(res[1])!=0 and res[1]<rem and res[6]<0)]\n",
    "        # find the top 75th quantile for the riemann sum and only keep trendlines with the better than quantile\n",
    "        quant = pd.Series([r[6] for r in results]).quantile(0.35)\n",
    "        results = [res for res in results if res[6]<=quant]\n",
    "    else: return print('Need mm_str input to be \"min\" or \"max\".')\n",
    "    # now perform a sort and filter method to dwindle down the best trendline\n",
    "    # the process is sort by residuals, grab 30, sort by price difference, grab 20, sort by residuals again, grab 10\n",
    "    # finally sort by price difference\n",
    "    results.sort(key=lambda val:val[1])\n",
    "    results = results[:30]\n",
    "    results.sort(key=lambda val:val[5])\n",
    "    results = results[:20]\n",
    "    results.sort(key=lambda val:val[1])\n",
    "    results = results[:10]\n",
    "    results.sort(key=lambda val:val[5])\n",
    "    return results, cur_trend\n",
    "\n",
    "\n",
    "def get_trendline_data(data, source='Close'):\n",
    "    '''\n",
    "    source could also be HighClose/LowClose avgs \n",
    "    '''\n",
    "    ser1 = data[source]\n",
    "    ser2 = data.iloc[int((1/3)*len(data)):][source]\n",
    "    ser3 = data.iloc[int((2/3)*len(data)):][source]\n",
    "    results=[]\n",
    "    for i, ser in enumerate([ser1,ser2,ser3]):\n",
    "        mins, maxs = mins_maxs(ser,limit=(18*(3-i)/3)) # decreasing by 1/3\n",
    "        try: res_max, cur_trend = get_trendlines(ser, maxs, 'max')\n",
    "        except:\n",
    "            print('No resistance trendlines currently.')\n",
    "            res_max, cur_trend = [],0\n",
    "        # no need to store current trend twice\n",
    "        try: res_min, _ = get_trendlines(ser, mins, 'min')\n",
    "        except:\n",
    "            print('No support trendlines currently.')\n",
    "            res_min = []\n",
    "        if len(res_min)==0: res_min=[[]]\n",
    "        if len(res_max)==0: res_max=[[]]\n",
    "        results.append((res_min[0], res_max[0], cur_trend))\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_df(SYMBOL, data=None, first=True):\n",
    "    \n",
    "    global time_dict\n",
    "    \n",
    "    print('STEP1: Creating df object ...                                                  Time elapsed:',round(time.time()-st,2),'s')\n",
    "    try:\n",
    "        data, restart = intraday_data(symbol=SYMBOL, data=data, first=first)\n",
    "        if restart:\n",
    "            print('Had to restart, waiting til the top of next minute.')\n",
    "            while True:\n",
    "                if datetime.datetime.now().second <= 5:\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "            data, restart = intraday_data(symbol=SYMBOL,data=data, first=False, restart=restart)\n",
    "        df, bid, ask = data\n",
    "        \n",
    "    except:\n",
    "        e = sys.exc_info()\n",
    "        print('Error getting df...')\n",
    "        print(f'ValueError due to:\\n{e[0]}\\n{e[1]}')\n",
    "        raise\n",
    "    stoK, stoD = stochastic(df)\n",
    "    rsi = RSI(df)\n",
    "    bb = bollinger_bands(df)\n",
    "    print(df.shape, stoK.shape, rsi.shape, bb.shape)\n",
    "    print('STEP2: Adding stochastic, rsi and bollinger band indicators ...                Time elapsed:',round(time.time()-st,2),'s')\n",
    "    df = pd.concat([df,stoK,rsi,bb], axis=1)\n",
    "    df['ind__rsi_accel'] = ind_acceleration(df.ind__rsi)\n",
    "    df['ind__rsi_rolling'] = get_rolling(df.ind__rsi)\n",
    "    df['ind__stoK_accel'] = ind_acceleration(df.ind__stochK)\n",
    "    df['ind__stoK_rolling'] = get_rolling(df.ind__stochK)\n",
    "    print('STEP3: Perfomring bollinger band analysis and adding SMA slope indicator ...   Time elapsed:',round(time.time()-st,2),'s')\n",
    "    df = bb_analysis(df.copy())\n",
    "\n",
    "    return df.dropna(), bid, ask\n",
    "    \n",
    "\n",
    "def get_current_data(SYMBOL, data=None, first=True):\n",
    "    global past_str, BS_count, QS_count, recent_chg\n",
    "    \n",
    "    df, bid, ask = create_df(SYMBOL=SYMBOL, data=data, first=first)\n",
    "    print('STEP4: Creating current_data object for training data entry ...                Time elapsed:',round(time.time()-st,2),'s')   \n",
    "    \n",
    "    print('df info:',df[-1:].Close, df[-1:].index, df.shape)\n",
    "    current_data = df.loc[df[-1:].index[0]:].copy()\n",
    "    # add spread, bid-cur, ask-cur\n",
    "    current_data['spread'] = ask - bid\n",
    "    current_data['bid_diff'] = current_data['Close'][0] - bid\n",
    "    current_data['ask_diff'] = ask - current_data['Close'][0]\n",
    "    \n",
    "    print('STEP5: Performing trendline analysis and adding trendline indicators ...       Time elapsed:',round(time.time()-st,2),'s')\n",
    "    # handling trendline data\n",
    "    # determined by whether or not the price is going towards the trendline, and it is closer to the approaching trendline than\n",
    "    # the leaving trendline. if it is surpassed the closest trendline, is it heading back towards it, if so this could indicate\n",
    "    # leaving a treandline and bouncing back\n",
    "    def trendline_analysis(trends, line='sup',frame='full'):\n",
    "        for frame_type in [('full',0),('2_3',1),('1_3',2)]:\n",
    "            if frame==frame_type[0]:\n",
    "                tl_frame = trends[frame_type[1]]\n",
    "                tl_sup, tl_res, cur_dir = tl_frame\n",
    "                if len(tl_sup)>0 and len(tl_res)>0:\n",
    "                    if line=='sup' and tl_sup[5]<tl_res[5]: # first we check if the cur price is closer to support or resistance\n",
    "                        # check if price difference is < 0, meaning cur price is beyond support tl\n",
    "                        # then check if cur direction is moving back towards trendline\n",
    "                        if tl_sup[7]>0 and cur_dir<0: return 1\n",
    "                        elif tl_sup[7]<0 and cur_dir>0: return 1\n",
    "                        else: return 0\n",
    "                    elif line=='res' and tl_res[5]<tl_sup[5]:\n",
    "                        if tl_res[7]<0 and cur_dir>0: return 1\n",
    "                        elif tl_res[7]>0 and cur_dir<0: return 1\n",
    "                        else: return 0\n",
    "                    else: return 0\n",
    "                elif len(tl_sup)==0 and line=='sup': return 0\n",
    "                elif len(tl_res)==0 and line=='res': return 0\n",
    "                elif len(tl_sup)>0 or len(tl_res)>0:\n",
    "                    if line=='sup' and ((tl_sup[7]>0 and cur_dir<0) or (tl_sup[7]<0 and cur_dir>0)): return 1\n",
    "                    elif line=='res' and ((tl_res[7]<0 and cur_dir>0) or (tl_res[7]>0 and cur_dir<0)): return 1\n",
    "                    else: return 0\n",
    "                else: return 0\n",
    "                \n",
    "    trend_frame = get_trendline_data(df.copy(), source='Close')\n",
    "    current_data['ind__full_near_res'] = trendline_analysis(trend_frame, 'res', 'full')\n",
    "    current_data['ind__full_near_sup'] = trendline_analysis(trend_frame, 'sup', 'full')\n",
    "    current_data['ind__2_3_near_res'] = trendline_analysis(trend_frame, 'res', '2_3')\n",
    "    current_data['ind__2_3_near_sup'] = trendline_analysis(trend_frame, 'sup', '2_3')\n",
    "    current_data['ind__1_3_near_res'] = trendline_analysis(trend_frame, 'res', '1_3')\n",
    "    current_data['ind__1_3_near_sup'] = trendline_analysis(trend_frame, 'sup', '1_3')\n",
    "    \n",
    "    print('STEP6: Scraping livecharts.co website for currency strength indicators ...     Time elapsed:',round(time.time()-st,2),'s')\n",
    "    # create the currency strength indicators\n",
    "    try:\n",
    "        base_str, quote_str, str_rat, cur_dict = get_currency_strength(symbol=SYMBOL)\n",
    "    except:\n",
    "        e = sys.exc_info()\n",
    "        print(f'livecharts.co.uk/currency-strength.php unavailable for the moment:\\ndue to:\\n{e[0]}\\n{e[1]}')\n",
    "        base_str, quote_str, str_rat = (0,0,0)\n",
    "    \n",
    "    # when training the machine, we will perform feature engineering on the categorical data\n",
    "    current_data['ind__base_strength'] = base_str\n",
    "    current_data['ind__quote_strength'] = quote_str\n",
    "    current_data['ind__strength_ratio'] = str_rat\n",
    "    current_data['ind__BS_diff'] = base_str - past_str['base']\n",
    "    current_data['ind__QS_diff'] = quote_str - past_str['quote']\n",
    "    \n",
    "    # tracking recent big changes to currencies\n",
    "    if abs(base_str - past_str['base']) >= 2:\n",
    "        recent_chg['base'] = 1\n",
    "        BS_count = 0\n",
    "    else:\n",
    "        BS_count += 1\n",
    "        if BS_count >= 5:\n",
    "            recent_chg['base'] = 0\n",
    "    if abs(quote_str - past_str['quote']) >= 2:\n",
    "        recent_chg['quote'] = 1\n",
    "        QS_count = 0\n",
    "    else:\n",
    "        QS_count += 1\n",
    "        if QS_count >= 5:\n",
    "            recent_chg['quote'] = 0\n",
    "    current_data['ind__BS_pastchg'] = recent_chg['base']\n",
    "    current_data['ind__QS_pastchg'] = recent_chg['quote']\n",
    "    # set past strengths\n",
    "    past_str['base'] = base_str\n",
    "    past_str['quote'] = quote_str\n",
    "    \n",
    "    # get slopes and residuals\n",
    "    vals, _ = get_bestfit(df.ind__mid_bb, period=3)\n",
    "    current_data['ind__midbb_slope'] = vals[0]\n",
    "    # the intensions are to grab the sum of least squared value and use this to determine a quick trend\n",
    "    # pair it with the past 5 up_tick value to determine if it is going in one direction\n",
    "    _, resid = get_bestfit(df.Close, period=5)\n",
    "    current_data['ind__trend_residuals'] = resid[0]\n",
    "    \n",
    "    return current_data, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "Scraping Forex Factory website for news updates and how they affect the around the time of the events.\n",
    "\n",
    "Scraping livecharts website for currency strengths.\n",
    "\n",
    "[To top^^^](#Table-of-Contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# livecharts scraping function\n",
    "def get_currency_strength(symbol):\n",
    "    '''\n",
    "    For instance, if a certain currency is very strong, and another suddenly turns weaker, you may find a trading opportunity.\\\n",
    "Such deviation between pairs usually indicates momentum. Conversely, if two currencies are weak, strong or average strength,\\\n",
    "there is often a range or sideways movement happening. \n",
    "    '''\n",
    "    base = symbol[:3]\n",
    "    quote = symbol[3:]\n",
    "    \n",
    "    # EUR tick is EURO on website\n",
    "    if base == 'EUR': base='EURO'\n",
    "    if quote == 'EUR': quote='EURO'\n",
    "        \n",
    "    cur_dict = {}\n",
    "    \n",
    "    res = requests.get('http://www.livecharts.co.uk/currency-strength.php')\n",
    "    soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    txt = soup.select('#rate-outercontainer')\n",
    "    for item in txt:\n",
    "        strength = 6 - str(item).count('image:none')\n",
    "        currency = item.find(id='map-innercontainer-symbol').contents[0]\n",
    "        cur_dict[currency] = strength\n",
    "        \n",
    "    base_strength = cur_dict[base]\n",
    "    quote_strength = cur_dict[quote]\n",
    "    strength_ratio = quote_strength/base_strength\n",
    "    \n",
    "    return base_strength, quote_strength, strength_ratio, cur_dict\n",
    "# Done with livecharts scraping\n",
    "##\n",
    "### -----------------------------------------------------------------\n",
    "##\n",
    "# Forex Factory scraping beyond this point\n",
    "# we are looking at all calendar events for the week\n",
    "def find_eventid(soup):\n",
    "    '''\n",
    "    We are looking at all calendar events for the week. In this case, there are multiple classes that the 'data-eventid' can\\\n",
    "fall under, which is why we need to filter through all repeated values.\n",
    "    '''\n",
    "    past = 0\n",
    "    eventid = []\n",
    "    event_list = soup.find(class_=\"calendar__table\").find_all(class_='calendar__row')\n",
    "    for item in event_list:\n",
    "        if 'data-eventid' in item.attrs.keys():\n",
    "            id_ = item.attrs['data-eventid']\n",
    "            if id_ == past: pass\n",
    "            else: \n",
    "                eventid.append(id_)\n",
    "                past = id_\n",
    "    return eventid\n",
    "\n",
    "\n",
    "def event_details(event_id):\n",
    "    scraper = cloudscraper.create_scraper(interpreter='nodejs')\n",
    "    # the base url is neccessary since the web page is dynamic - it was found through an element analysis of the website\n",
    "    base_url = 'https://www.forexfactory.com/flex.php?do=ajax&contentType=Content&flex=calendar_mainCalCopy1&details='\n",
    "    eventid_url = event_id\n",
    "    detailed_url = base_url+eventid_url\n",
    "    res = scraper.get(detailed_url)\n",
    "    soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    \n",
    "    # create a all list/dictionary variables\n",
    "    stories, specs, spec_content = [], [], []\n",
    "    spec_dict = {}\n",
    "    \n",
    "    for item in soup.find_all(class_=\"flexposts__story-title\"):\n",
    "        stories.append(item.select('a')[0].attrs['title'])\n",
    "    stories\n",
    "    for item in soup.find_all('tr'):\n",
    "        for x in item.find_all(class_='label calendarspecs__spec'):\n",
    "            specs.append(x.contents[0].replace('\\n','').replace('\\t',''))\n",
    "    specs\n",
    "    for item in soup.find_all('tr'):\n",
    "        for x in item.find_all(class_='full calendarspecs__specdescription'):\n",
    "            spec_content.append(x.contents[0])\n",
    "    spec_content\n",
    "    spec_dict={specs[i]:spec_content[i] for i in range(len(specs))}\n",
    "    return stories, spec_dict\n",
    "\n",
    "\n",
    "def event_impact(soup):\n",
    "    impact = []\n",
    "    for item in soup.find(class_=\"calendar__table\").find_all(class_='calendar__impact'):\n",
    "        if len(item.select('span')) > 0:\n",
    "            it = item.select('span')[0].attrs\n",
    "            impact.append(it['class'][0])\n",
    "    return impact\n",
    "\n",
    "\n",
    "def currency_impacted(soup):\n",
    "    currency = []\n",
    "    symbols = soup.find(class_=\"calendar__table\").find_all(class_=\"calendar__cell calendar__currency currency\")\n",
    "    for item in symbols:\n",
    "        clean = item.contents[0].replace('\\n','')\n",
    "        currency.append(clean)\n",
    "    return currency\n",
    "\n",
    "\n",
    "def find_date(soup, year='2020'):\n",
    "    date = soup.find(class_='date').find(class_='date').contents[1].contents[0]\n",
    "    time = soup.find(class_=\"calendar__table\").find(title='Time Options').contents[0]\n",
    "    datetime_str = date+' '+year+' '+time\n",
    "    datetime_obj = datetime.datetime.strptime(datetime_str, '%b %d %Y %I:%M%p')\n",
    "    return datetime_obj\n",
    "\n",
    "\n",
    "def find_event_time(soup, date):\n",
    "    # parse all times per event and store in list\n",
    "    times = []\n",
    "    results = []\n",
    "    find_date = soup.find(class_=\"calendar__table\").find_all(class_='calendar__cell calendar__date date')\n",
    "    # get the month_day into the same form that it is returned by the above cell\n",
    "    now = datetime.datetime.now()\n",
    "    month_day = ' '.join([month_abbr[now.month], str(now.day)])\n",
    "    for i,time in enumerate(soup.find(class_=\"calendar__table\").find_all(class_='calendar__cell calendar__time time')):\n",
    "        if len(time.contents)==0: times.append(last)\n",
    "        else:\n",
    "            if len(find_date[i]) > 1: month_day = find_date[i].contents[1].contents[1].contents[0]\n",
    "            typ = type(time.contents[0])\n",
    "            item = time.contents\n",
    "            if typ == bs4.element.NavigableString:\n",
    "                times.append((month_day, item[0]))\n",
    "                last = (month_day, item[0])\n",
    "            elif typ == bs4.element.Tag:\n",
    "                times.append((month_day, item[1].contents[1]))\n",
    "                last = (month_day, item[1].contents[1])\n",
    "    for time in times:\n",
    "        if time[1] == 'All Day': result = time[1]\n",
    "        elif time[1] == 'Tentative': result = time[1]\n",
    "        elif ':' in time[1]:\n",
    "            spl_m_d = time[0].split()\n",
    "            month = spl_m_d[0]\n",
    "            day = spl_m_d[1]\n",
    "            date_ = ' '.join([month,day,str(date.year)])\n",
    "            datetime_str = ' '.join([date_, time[1]])\n",
    "            datetime_obj = datetime.datetime.strptime(datetime_str, '%b %d %Y %I:%M%p')\n",
    "            result = datetime_obj\n",
    "        else: result='N/A'\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def time_until_event(event_times, current_time):\n",
    "    event_time_list = []\n",
    "    for time in event_times:\n",
    "        if type(time) == datetime.datetime:\n",
    "            delta = time - current_time\n",
    "            if delta.days >= 0:\n",
    "                mins_until_event = delta.seconds / 60\n",
    "                event_time_list.append(mins_until_event)\n",
    "            else: event_time_list.append(('past', (current_time-time).seconds/60))# return a tuple showing how recent event was\n",
    "        else: event_time_list.append(time)\n",
    "    return event_time_list\n",
    "\n",
    "\n",
    "def detail_collection(use_symbols):\n",
    "    detail_dict = {}\n",
    "    for i, sym, id_ in use_symbols:\n",
    "        detail_dict[f'{sym}_{i}'] = event_details(id_)\n",
    "    return detail_dict\n",
    "\n",
    "\n",
    "def forecast_indicator(soup, detail_dict, use_symbols):\n",
    "    \n",
    "    def usual_effect(detail_dict):\n",
    "        '''\n",
    "        1 - 'Forecast' greater than 'Previous' good for currency\n",
    "        0 - 'Forecast' less than 'Previous' good for currency\n",
    "        'none' - Nothing to find\n",
    "        '''\n",
    "        effect = []\n",
    "        for key in detail_dict.keys():\n",
    "            details = detail_dict[key][1]\n",
    "            if 'Usual Effect' in details.keys():\n",
    "                desc = details['Usual Effect']\n",
    "                if 'greater' in desc:\n",
    "                    # meaning 'Actual' greater than 'Forecast' is GOOD for currency\n",
    "                    # we will use this to identify if 'forecast' > 'previous' does anything to the market\n",
    "                    effect.append(1)\n",
    "                elif 'less' in desc: effect.append(0)\n",
    "                else: effect.append('none')\n",
    "            else: effect.append('none')\n",
    "        return effect\n",
    "    \n",
    "    def find_previous_forecast(soup, use_symbols, prev_fore='previous'):\n",
    "        '''\n",
    "        Only two arguments for prev_fore are 'previous' and 'forecast'.\n",
    "        '''\n",
    "        index_vals = [i for i,sym,id_ in use_symbols]\n",
    "        val_list = []\n",
    "        find_str = f'calendar__cell calendar__{prev_fore} {prev_fore}'\n",
    "        for item in soup.find(class_=\"calendar__table\").find_all(class_=find_str):\n",
    "            cont = item.contents\n",
    "            if len(cont)==0: val_list.append('none')\n",
    "            elif len(cont)==1:\n",
    "                typ = type(cont[0])\n",
    "                if typ == bs4.element.Tag:\n",
    "                    revised = cont[0].attrs['class']\n",
    "                    if len(revised)>1:\n",
    "                        outcome = revised[1]\n",
    "                        val_list.append((outcome, cont[0].contents[0]))\n",
    "                    else: val_list.append(cont[0].contents[0])\n",
    "                elif typ == bs4.element.NavigableString: val_list.append(cont[0])\n",
    "            else: val_list.append('none')\n",
    "        return [val_list[i] for i in index_vals]\n",
    "\n",
    "    def forecast_predict(prev_vals, fore_vals, rules):\n",
    "        direction = []\n",
    "        for i, pre in enumerate(prev_vals):\n",
    "            if type(pre) == tuple: pre_ = pre\n",
    "            else: pre_ = ('none',pre)\n",
    "            if rules[i] == 'none' or pre_[1] == 'none' or fore_vals[i] == 'none': direction.append(('none','none'))\n",
    "            elif rules[i] == 0:\n",
    "                if pre_[1] - fore_vals[i] > 0: direction.append((pre_[0],'right'))\n",
    "                elif pre_[1] - fore_vals[i] < 0: direction.append((pre_[0],'wrong'))\n",
    "                else: direction.append((pre_[0],'same'))\n",
    "            elif rules[i] == 1:\n",
    "                if fore_vals[i] - pre_[1] > 0: direction.append((pre_[0],'right'))\n",
    "                elif fore_vals[i] - pre_[1] < 0: direction.append((pre_[0],'wrong'))\n",
    "                else: direction.append((pre_[0],'same'))\n",
    "        return direction\n",
    "    \n",
    "    def get_floats(val_list):\n",
    "        float_vals = []\n",
    "        for val in val_list:\n",
    "            try:\n",
    "                float_vals.append(float(val))\n",
    "            except:\n",
    "                if val == 'none': float_vals.append(val)\n",
    "                elif type(val)==tuple: float_vals.append((val[0], float(val[1][:-1])))\n",
    "                else:\n",
    "                    try:\n",
    "                        float_vals.append(float(val[:-1]))\n",
    "                    except:\n",
    "                        float_vals.append('none')\n",
    "        return float_vals\n",
    "    \n",
    "    rules = usual_effect(detail_dict)\n",
    "    prev_vals = get_floats(find_previous_forecast(soup, use_symbols, prev_fore='previous'))\n",
    "    fore_vals = get_floats(find_previous_forecast(soup, use_symbols, prev_fore='forecast'))\n",
    "    \n",
    "    return forecast_predict(prev_vals, fore_vals, rules)\n",
    "    \n",
    "    \n",
    "def find_actual(soup, time_til_event):\n",
    "    actual = []\n",
    "    for i,item in enumerate(soup.find(class_=\"calendar__table\").find_all(class_='calendar__cell calendar__actual actual')):\n",
    "        cont = item.contents\n",
    "        if len(cont)==0:\n",
    "            if type(time_til_event[i])==tuple: actual.append('none')\n",
    "            else: actual.append('still to come')\n",
    "        elif len(cont)==1:\n",
    "            cont = cont[0]\n",
    "            typ = type(cont)\n",
    "            if typ == bs4.element.Tag:\n",
    "                outcome = cont.attrs['class'][0]\n",
    "                actual.append(outcome)\n",
    "            elif typ == bs4.element.NavigableString: actual.append('neutral')\n",
    "        else: actual.append('none')\n",
    "    return actual\n",
    "\n",
    "\n",
    "def get_ff_alert(detail_dict):\n",
    "    alerts = []\n",
    "    for event in detail_dict.keys():\n",
    "        if 'FF Alert' in list(detail_dict[event][1].keys()): alerts.append(1)\n",
    "        else: alerts.append(0)\n",
    "    return alerts\n",
    "\n",
    "\n",
    "def symbols_to_use(symbol, event_ids, currencies):\n",
    "    '''\n",
    "    returns a list of index values to source data-eventids\n",
    "    '''\n",
    "    base, quote = symbol[:3], symbol[3:]\n",
    "    return [(i,sym, event_ids[i]) for i, sym in enumerate(currencies) if sym in [base, quote]]\n",
    "\n",
    "\n",
    "def event_sentiment(detail_dict):\n",
    "    \n",
    "    def get_sentiment(comment):\n",
    "        cleaned = text_process(comment, tokenize=False)\n",
    "        sentiment = TextBlob(cleaned).sentiment\n",
    "        return sentiment\n",
    "    \n",
    "    sentiments = []\n",
    "    for key in detail_dict.keys():\n",
    "        stories = detail_dict[key][0]\n",
    "        details = detail_dict[key][1]\n",
    "        polarity = []\n",
    "        pol_neutral = []\n",
    "        if len(stories) != 0:\n",
    "            for story in stories:\n",
    "                sent = get_sentiment(story)\n",
    "                pol, subj = sent.polarity, sent.subjectivity\n",
    "                if subj<0.7 and pol>0: polarity.append('positive')\n",
    "                elif subj<0.7 and pol<0: polarity.append('negative')\n",
    "                elif subj<0.7 and pol==0: pol_neutral.append(1)\n",
    "        for spec in details.keys():\n",
    "            if 'traders' in spec.lower() or 'ff notes' in spec.lower():\n",
    "                sent = get_sentiment(details[spec])\n",
    "                pol, subj = sent.polarity, sent.subjectivity\n",
    "                if subj<0.7 and pol>0: polarity.append('positive')\n",
    "                elif subj<0.7 and pol<0: polarity.append('negative')\n",
    "                elif subj<0.7 and pol==0: pol_neutral.append(1)\n",
    "        sentiment = (dict(count(polarity)), np.sum(pol_neutral))\n",
    "        pol, neut = sentiment\n",
    "        if len(pol) > 0 and neut == 0:\n",
    "            if len(pol) == len(np.unique(pol)): sentiments.append(max(pol.items(), key=operator.itemgetter(1))[0])\n",
    "            else: sentiments.append('neutral')\n",
    "        elif len(pol) > 0 and neut > 0:\n",
    "            vals = list(pol.values())\n",
    "            if neut > max(vals): sentiments.append('neutral')\n",
    "            elif len(pol) == len(np.unique(pol)): sentiments.append(max(pol.items(), key=operator.itemgetter(1))[0])\n",
    "            else: sentiments.append('neutral')\n",
    "        else: sentiments.append('neutral')\n",
    "    return sentiments\n",
    "\n",
    "\n",
    "# this is the only function that we are pulling from\n",
    "def FF_main_scrape(symbol):\n",
    "    # scope the url to only view today and one day in advance\n",
    "    from calendar import month_abbr, day_name\n",
    "    now = datetime.datetime.now()\n",
    "    month = month_abbr[now.month].lower()\n",
    "    tod = '.'.join([(month+str(now.day)), str(now.year)])\n",
    "    if day_name[now.weekday()]=='Friday': date_str = ''.join(['day=',tod])\n",
    "    else:\n",
    "        nextnow = now + datetime.timedelta(1) # add a day\n",
    "        month = month_abbr[nextnow.month].lower()\n",
    "        tom = '.'.join([(month+str(nextnow.day)), str(nextnow.year)])\n",
    "        date_str = ''.join(['range=',tod,'-',tom])\n",
    "                \n",
    "    base_pre = 'https://www.forexfactory.com/calendar?'\n",
    "    search_url = ''.join([base_pre,date_str])\n",
    "    try:\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        res = scraper.get(search_url)\n",
    "        soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    except:\n",
    "        print('Trying soup again...')\n",
    "        time.sleep(2)\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        res = scraper.get(search_url)\n",
    "        soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    print('Got the soup.')\n",
    "    scrape_date = find_date(soup)\n",
    "    event_times = find_event_time(soup, scrape_date) # maybe useful\n",
    "    time_til_events = time_until_event(event_times, scrape_date) # useful\n",
    "    # pull currencies and event ids\n",
    "    currencies = currency_impacted(soup) # symbol\n",
    "    event_ids = find_eventid(soup)\n",
    "    use_symbols = symbols_to_use(symbol, event_ids, currencies)\n",
    "    # pull impact indication\n",
    "    impact = event_impact(soup) # indicator\n",
    "    actuals = find_actual(soup, time_til_events) # indicator\n",
    "    detail_dict = detail_collection(use_symbols)\n",
    "    forecasts = forecast_indicator(soup, detail_dict, use_symbols) # indicator\n",
    "    alerts = get_ff_alert(detail_dict) # indicator\n",
    "    sentiments = event_sentiment(detail_dict) # indicator\n",
    "    \n",
    "    store_dict = {}\n",
    "    result_dict = {}\n",
    "\n",
    "    for n,sym_tup in enumerate(use_symbols):\n",
    "        i,sym,_ = sym_tup\n",
    "        store_dict[sym+'_'+str(i)] ={\n",
    "            'event_time':event_times[i], \n",
    "            'time_until':time_til_events[i],\n",
    "            'impact_level':impact[i],\n",
    "            'actual_result':actuals[i],\n",
    "            'forecast':forecasts[n],\n",
    "            'ff_alert':alerts[n],\n",
    "            'event_sentiment':sentiments[n]\n",
    "        }\n",
    "    for key in store_dict.keys():\n",
    "        cur = key.split('_')[0]\n",
    "        if cur not in result_dict.keys(): result_dict[cur] = [store_dict[key]]\n",
    "        else: result_dict[cur].append(store_dict[key])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER SENTIMENT ANALYSIS\n",
    "Performs a sentiment analysis of public tweets and trader success over the past XX number of tweets\n",
    "\n",
    "[To top^^^](#Table-of-Contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(comment, tokenize=True):\n",
    "    # strip emojis/pictographs/symbols/etc...\n",
    "    comment = comment.encode('ascii', 'ignore').decode('ascii')\n",
    "    # create tokens to clean\n",
    "    tokens = comment.split(' ')\n",
    "    cleaned_tokens = []\n",
    "    # clean out any url and user tag\n",
    "    for token in tokens:\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        # take out all numbers\n",
    "        token = re.sub('\\d+','',token)\n",
    "        cleaned_tokens.append(token)\n",
    "    cleaned_tokens = ' '.join(cleaned_tokens)\n",
    "    # lemmatization - like stemming, trying to get the word to its root\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in cleaned_tokens]\n",
    "    # remove punctuation\n",
    "    nopunc = [char for char in lemmatized if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    # for tokenization, take away the 'join' method\n",
    "    if tokenize: return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n",
    "\n",
    "\n",
    "# dictionary containing twitter api access tokens\n",
    "access_dict = {\n",
    "               'access_1':{'API_KEY':'',\n",
    "                          'API_SECRET':'',\n",
    "                          'ACCESS_TOKEN':'',\n",
    "                          'ACCESS_TOKEN_SECRET':''},\n",
    "               'access_2':{'API_KEY':'',\n",
    "                          'API_SECRET':'',\n",
    "                          'ACCESS_TOKEN':'',\n",
    "                          'ACCESS_TOKEN_SECRET':''},\n",
    "              }\n",
    "\n",
    "def set_api(key, keys):\n",
    "    API_KEY = access_dict[key]['API_KEY']\n",
    "    API_SECRET = access_dict[key]['API_SECRET']\n",
    "    ACCESS_TOKEN = access_dict[key]['ACCESS_TOKEN']\n",
    "    ACCESS_TOKEN_SECRET = access_dict[key]['ACCESS_TOKEN_SECRET']\n",
    "    auth = tw.OAuthHandler(API_KEY, API_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "    api = tw.API(auth)\n",
    "    # check status of api\n",
    "    status = api.rate_limit_status()['resources']['search']['/search/tweets']\n",
    "    remaining, next_update = status['remaining'], status['reset']\n",
    "    # check if api\n",
    "    if remaining <= 30:\n",
    "        print('Changing api access tokens. Resetting in',round((next_update-time.time())/60, 2),'minutes\\n')\n",
    "        keys_ = keys\n",
    "        keys_.remove(key)\n",
    "        key = keys_[0]\n",
    "        api = set_api(key, keys)\n",
    "    return api    \n",
    "\n",
    "\n",
    "def minute_tweet_freq(symbols, current_time, num_tweets=150):\n",
    "    '''\n",
    "    pass in symbols as a list\n",
    "    200 tweets was roughly an hour of tweet backlog for 'EURUSD'\n",
    "    \n",
    "    We will use this function to create our own NLP sentiment analysis.\n",
    "    The process will be roughly collect tweets, collect past intraday data, and find a correlation between words and market \\\n",
    "trends\n",
    "    '''\n",
    "    \n",
    "    # set api with first account in dict\n",
    "    keys = list(access_dict.keys())\n",
    "    api = set_api(keys[0], keys)\n",
    "    for symbol in symbols:    \n",
    "        # when using tweet_mode='extended', the 'text' attribute becomes 'full_text'\n",
    "        tm='extended'\n",
    "        search_words = symbol\n",
    "        public_tweets = tw.Cursor(api.search, q=search_words, lang='en', tweet_mode=tm).items(num_tweets)\n",
    "        # time difference is 6 hour for time zone, and subtract 60 seconds for past minute of tweets\n",
    "        past_minute = (current_time+datetime.timedelta(0,21540)).strftime('%Y-%m-%d %H:%M')\n",
    "        created = [[tweet.created_at] for tweet in public_tweets if tweet.created_at > pd.Timestamp(past_minute)]\n",
    "        # df = pd.DataFrame(data=created, columns=[\"created_at\"])\n",
    "\n",
    "    return len(created)\n",
    "\n",
    "\n",
    "# function for machine\n",
    "def twitter_sentiment(symbols, current_time, num_tweets=350):\n",
    "    '''\n",
    "    pass in symbols as a list\n",
    "    '''\n",
    "\n",
    "    # set api with first account in dict\n",
    "    keys = list(access_dict.keys())\n",
    "    api = set_api(keys[0], keys)\n",
    "    # set variables\n",
    "    past_tweet = str()\n",
    "    public_sentiment, trader_sentiment = [], []\n",
    "\n",
    "    for symbol in symbols:    \n",
    "        # when using tweet_mode='extended', the 'text' attribute becomes 'full_text'\n",
    "        tm='extended'\n",
    "        search_words = symbol\n",
    "        public_tweets = tw.Cursor(api.search, q=search_words, lang='en', tweet_mode=tm).items(num_tweets)\n",
    "        # time difference is 6 hour for time zone, and subtract 60 seconds for past minute of tweets\n",
    "        past_minute = (current_time+datetime.timedelta(0,21540)).strftime('%Y-%m-%d %H:%M')\n",
    "        # collect past 2 hours of tweets to create sentiment from\n",
    "        past_2hrs = (current_time+datetime.timedelta(0,14400)).strftime('%Y-%m-%d %H:%M')\n",
    "        # counting amount of tweets\n",
    "        recent_tweets = [[tweet.full_text, tweet.created_at] for tweet in public_tweets if tweet.created_at>=pd.Timestamp(past_2hrs)]\n",
    "        tweet_freq = len([tweet for tweet in recent_tweets if tweet[1]>=pd.Timestamp(past_minute)])\n",
    "        results, trade_results, clean_tweets = [],[],[]\n",
    "        # sentiment analysis\n",
    "        for tweet in recent_tweets:\n",
    "            text = tweet[0]\n",
    "            # we will use the 'match' to determine if a trader won or lost a position\n",
    "            # these traders were found during the analysis, a tweet whenever a position is closed\n",
    "            # assuming the position was taked off of an analysis, the win or loss can help us identify sentiment\n",
    "            match = re.findall(fr'Closed \\w+[ \\w+ ]?\\w+ \\d+.\\d+ Lots [$#]?{symbol}|Closed \\w+ {symbol}', text)\n",
    "            if len(match)>0:\n",
    "                buy_sell = re.findall('(Buy|Sell)', text)\n",
    "                win_lose = re.findall('for \\W?', text)[0].split()\n",
    "                if len(win_lose) > 1:\n",
    "                    outcome = win_lose[1]\n",
    "                    if outcome == '+': trade_results.append(1)\n",
    "                    elif outcome == '-': trade_results.append(-1)\n",
    "                else: trade_results.append(-1)\n",
    "                # we dont want to include this in the sentiment analysis so we will just continue to the next tweet\n",
    "                continue\n",
    "            # checking to see if a bot RT multiple times (found during analysis)\n",
    "            if text == past_tweet: continue\n",
    "            else:\n",
    "                past_tweet = text\n",
    "                cleanedtext = text_process(text, tokenize=False)\n",
    "                # checking if a RT came up again from the same user\n",
    "                if cleanedtext in clean_tweets:\n",
    "                    continue\n",
    "                else:\n",
    "                    clean_tweets.append(cleanedtext)\n",
    "                    sentiment = TextBlob(cleanedtext).sentiment\n",
    "                    pol, subj = sentiment.polarity, sentiment.subjectivity\n",
    "                    if subj < 0.7:\n",
    "                        if pol > 0: results.append(1)\n",
    "                        elif pol < 0: results.append(-1)\n",
    "                        else: results.append(0)\n",
    "                            \n",
    "        res = np.sum(results)/len(results)\n",
    "        if res > 0 and abs(res) > 0.1: public_sentiment.append(('positive', res))\n",
    "        elif res < 0 and abs(res) > 0.1: public_sentiment.append(('negative', res))\n",
    "        else: public_sentiment.append(('neutral', res))\n",
    "        if len(trade_results) != 0:\n",
    "            tr_res = np.sum(trade_results)/len(trade_results)\n",
    "            if tr_res > 0 and abs(tr_res) > 0.1: trader_sentiment.append(('positive', tr_res))\n",
    "            elif tr_res < 0 and abs(tr_res) > 0.1: trader_sentiment.append(('negative', tr_res))\n",
    "            else: trader_sentiment.append(('neutral', tr_res))\n",
    "        else: trader_sentiment.append(('none',0))\n",
    "        \n",
    "    return public_sentiment, trader_sentiment, tweet_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAMINING FUNCTION\n",
    "\n",
    "[To top^^^](#Table-of-Contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(SYMBOL, pos_time_int=3.0):\n",
    "    '''\n",
    "    pos_time_int == position time interval - indicates actual time interval used on Videforex\n",
    "    '''\n",
    "    global df, entry\n",
    "    global past_str, BS_count, QS_count, recent_chg\n",
    "    global st\n",
    "    global event_time_dict\n",
    "    \n",
    "    # identify base/quote currencies\n",
    "    base_cur = SYMBOL[:3]\n",
    "    quote_cur = SYMBOL[3:]\n",
    "    # create empty dictionary for tracking currency strength\n",
    "    try:\n",
    "        base_str, quote_str, str_rat, cur_dict = get_currency_strength(symbol=SYMBOL)\n",
    "    except:\n",
    "        e = sys.exc_info()\n",
    "        print(f'livecharts.co.uk/currency-strength.php unavailable for the moment:\\ndue to:\\n{e[0]}\\n{e[1]}')\n",
    "        base_str, quote_str, str_rat = (0,0,0)\n",
    "    past_str = {'base':base_str,'quote':quote_str}\n",
    "    BS_count = 0\n",
    "    QS_count = 0\n",
    "    recent_chg = {'base':0, 'quote':0}\n",
    "    # create empty dict for appending results\n",
    "    store_entry = {}\n",
    "    # create empty dict for adding FF event times and adding arbitrary large value to key 'update_time' for initial loop\n",
    "    event_time_dict = {}\n",
    "    # create empty dict for storing time to update twitter sentiment\n",
    "    twitter_time = {}\n",
    "    # initialize loop count\n",
    "    i = 0\n",
    "    \n",
    "    print('CURRENCY PAIR:',SYMBOL,'\\n')\n",
    "    while True:\n",
    "        st = time.time()\n",
    "        print('START TIME:', datetime.datetime.now())\n",
    "        if i==0:\n",
    "            entry, df = get_current_data(SYMBOL=SYMBOL, data=None, first=True)\n",
    "            # this is where we will create first instance of FF data\n",
    "        else: entry, df = get_current_data(SYMBOL=SYMBOL, data=df.copy(), first=False)\n",
    "        # twitter sentiment analysis\n",
    "        # each symbol mined is roughly 5 seconds at 150 tweets\n",
    "        # we can plan to pull data from this every 5 minutes\n",
    "        if i==0:\n",
    "            twitter_time['twitter_start'] = datetime.datetime.now()\n",
    "            print('TWITTER SENTIMENT ANALYSIS BEGINING ...                                        Time elapsed:',round((time.time() - st), 2),'s')\n",
    "            twit_sent_pub, twit_sent_trad, freq = twitter_sentiment([SYMBOL], datetime.datetime.now(), num_tweets=350)\n",
    "        else:\n",
    "            try:\n",
    "                secs = 900 # 15 minutes\n",
    "                if (datetime.datetime.now() - twitter_time['twitter_start']).seconds > secs: # 15 minutes\n",
    "                    twitter_time['twitter_start'] = twitter_time['twitter_start'] + datetime.timedelta(0,secs)\n",
    "                    print('TWITTER SENTIMENT ANALYSIS BEGINING ...                                        Time elapsed:',round((time.time() - st), 2),'s')\n",
    "                    # 200 tweets is a little over an hour of tweets - on average?\n",
    "                    try: twit_sent_pub, twit_sent_trad, freq = twitter_sentiment([SYMBOL], datetime.datetime.now(), num_tweets=350)\n",
    "                    except:\n",
    "                        print('Twitter not available.')\n",
    "                        twit_sent_pub, twit_sent_trad, freq = [(0,0)],[(0,0)],0\n",
    "                else:\n",
    "                    print('Geting tweet frequency over the last minute.')\n",
    "                    freq = minute_tweet_freq([SYMBOL], datetime.datetime.now(), num_tweets=150)\n",
    "            except: \n",
    "                print('Twitter unavailable.')\n",
    "                pass\n",
    "        # we are only going to record the numerical polarity summation\n",
    "        entry['ind__pub_twit_pol'] = twit_sent_pub[0][1]\n",
    "        entry['ind__trad_twit_pol'] = twit_sent_trad[0][1]\n",
    "        entry['ind__numtweets_1min'] = freq\n",
    "        print('DONE ...                                                                       Time elapsed:',round((time.time() - st), 2),'s')\n",
    "        \n",
    "# -----------------------------------------------------------------------------------------------------------------------------        \n",
    "        \n",
    "        # FF data pull - keep this in here to keep 'event_time_dict' local\n",
    "        ## ACCESS CURRENTLY DENIED - negative\n",
    "        # ran into a 1020 Error where my IP was blocked, should look at rotating IP addresses or getting on a VPN\n",
    "        \n",
    "        # maybe make initial pull a function to use in 'else' statement as well\n",
    "        # WE SHOULD HAVE THIS IF STATEMENT CHECK IF THERE IS INFORMATION IN event_time_dict...\n",
    "        if i==0:\n",
    "            # if we dont do anything with the time below, just replace in the print statement with datetime object\n",
    "            print('SCRAPING FOREX FACTORY WEBSITE ...                                             Time elapsed:',round((time.time() - st), 2),'s')\n",
    "            FF_dict = FF_main_scrape(SYMBOL)\n",
    "            try: base_events = FF_dict[base_cur]\n",
    "            except: base_events = 'empty'\n",
    "            try: quote_events = FF_dict[quote_cur]\n",
    "            except: quote_events = 'empty'\n",
    "            # time until next events\n",
    "            try:\n",
    "                event_time_dict['next_event_base'] = min([time for time in [base['time_until'] for base in base_events] \\\n",
    "                                                      if type(time) in [int, float]])\n",
    "            except:\n",
    "                print('No more events in near future for base currency %s' % base_cur)\n",
    "                event_time_dict['next_event_base'] = 'none'\n",
    "            try:\n",
    "                event_time_dict['past_event_base'] = min([time[1] for time in [base['time_until'] for base in base_events] \\\n",
    "                                                     if type(time)==tuple])\n",
    "            except: \n",
    "                print('No past events for base currency %s' % base_cur)\n",
    "                event_time_dict['past_event_base'] = 'none'\n",
    "            try:\n",
    "                event_time_dict['next_event_quote'] = min([time for time in [quote['time_until'] for quote in quote_events] \\\n",
    "                                                       if type(time) in [int, float]])\n",
    "            except:\n",
    "                print('No more events in near future for quote currency %s' % quote_cur)\n",
    "                event_time_dict['next_event_quote'] = 'none'\n",
    "            try:\n",
    "                event_time_dict['past_event_quote'] = min([time[1] for time in [quote['time_until'] for quote in quote_events] \\\n",
    "                                                     if type(time)==tuple])\n",
    "            except:\n",
    "                print('No past events for quote currency %s' % quote_cur)\n",
    "                event_time_dict['past_event_quote'] = 'none'\n",
    "            # set time for updating scrape next\n",
    "            try:\n",
    "                if type(event_time_dict['next_event_base']) in [float, int] and type(event_time_dict['next_event_quote']) in [float, int]:\n",
    "                    event_time_dict['update_time'] = min(event_time_dict['next_event_base'],event_time_dict['next_event_quote'])\n",
    "                elif type(event_time_dict['next_event_base']) in [float, int]:\n",
    "                    event_time_dict['update_time'] = event_time_dict['next_event_base']\n",
    "                elif type(event_time_dict['next_event_quote']) in [float, int]:\n",
    "                    event_time_dict['update_time'] = event_time_dict['next_event_quote']\n",
    "                else:\n",
    "                    print('No upcoming events ... %s' % SYMBOL)\n",
    "                    event_time_dict['update_time'] = 'none'\n",
    "            except:\n",
    "                print('No upcoming events ... %s' % SYMBOL)\n",
    "                event_time_dict['update_time'] = 'none'\n",
    "        else:\n",
    "            # subtract minute from event times because we will have waited 1 minute since updating dataframe\n",
    "            if event_time_dict['update_time'] != 'none': event_time_dict['update_time'] -= 1\n",
    "            if event_time_dict['next_event_quote'] != 'none': event_time_dict['next_event_quote'] -= 1\n",
    "            if event_time_dict['past_event_quote'] != 'none': event_time_dict['past_event_quote'] += 1\n",
    "            if event_time_dict['next_event_base'] != 'none': event_time_dict['next_event_base'] -= 1\n",
    "            if event_time_dict['past_event_base'] != 'none': event_time_dict['past_event_base'] += 1\n",
    "            # create a method around when to search for this, near a currency symbol effected time\n",
    "            print('Minutes until next event: ',event_time_dict['update_time'])\n",
    "            # time is in MINUTES until next event\n",
    "            if event_time_dict['update_time'] == 'none':\n",
    "                print('No upcoming events ... %s' % SYMBOL)\n",
    "                #\n",
    "                #\n",
    "                # we should set a timer here to call function FF_main_scrape() after however many hours we want\n",
    "                #\n",
    "                #\n",
    "                pass                \n",
    "            elif event_time_dict['update_time']==10 or event_time_dict['update_time']<0:\n",
    "                # reset all times for next event coming up\n",
    "                print('SCRAPING FOREX FACTORY WEBSITE ...                                         Time elapsed:',round((time.time() - st), 2),'s')\n",
    "                FF_dict = FF_main_scrape(SYMBOL)\n",
    "                base_events = FF_dict[base_cur]\n",
    "                quote_events = FF_dict[quote_cur]\n",
    "                # time until next events\n",
    "                try:\n",
    "                    event_time_dict['next_event_base'] = min([time for time in [base['time_until'] for base in base_events] \\\n",
    "                                                          if type(time) in [int, float]])\n",
    "                except:\n",
    "                    print('No more events in near future for base currency %s' % base_cur)\n",
    "                    event_time_dict['next_event_base'] = 'none'\n",
    "                try:\n",
    "                    event_time_dict['past_event_base'] = min([time[1] for time in [base['time_until'] for base in base_events] \\\n",
    "                                                         if type(time)==tuple])\n",
    "                except: \n",
    "                    print('No past events for base currency %s' % base_cur)\n",
    "                    event_time_dict['past_event_base'] = 'none'\n",
    "                try:\n",
    "                    event_time_dict['next_event_quote'] = min([time for time in [quote['time_until'] for quote in quote_events] \\\n",
    "                                                           if type(time) in [int, float]])\n",
    "                except:\n",
    "                    print('No more events in near future for quote currency %s' % quote_cur)\n",
    "                    event_time_dict['next_event_quote'] = 'none'\n",
    "                try:\n",
    "                    event_time_dict['past_event_quote'] = min([time[1] for time in [quote['time_until'] for quote in quote_events] \\\n",
    "                                                         if type(time)==tuple])\n",
    "                except:\n",
    "                    print('No past events for quote currency %s' % quote_cur)\n",
    "                    event_time_dict['past_event_quote'] = 'none'\n",
    "                # set time for updating scrape next\n",
    "                try:\n",
    "                    if type(event_time_dict['next_event_base']) in [float, int] and type(event_time_dict['next_event_quote']) in [float, int]:\n",
    "                        event_time_dict['update_time'] = min(event_time_dict['next_event_base'],event_time_dict['next_event_quote'])\n",
    "                    elif type(event_time_dict['next_event_base']) in [float, int]:\n",
    "                        event_time_dict['update_time'] = event_time_dict['next_event_base']\n",
    "                    elif type(event_time_dict['next_event_quote']) in [float, int]:\n",
    "                        event_time_dict['update_time'] = event_time_dict['next_event_quote']\n",
    "                    else:\n",
    "                        print('No upcoming events ... %s' % SYMBOL)\n",
    "                        event_time_dict['update_time'] = 'none'\n",
    "                except:\n",
    "                    print('No upcoming events ... %s' % SYMBOL)\n",
    "                    event_time_dict['update_time'] = 'none'\n",
    "        # we shouldn't have conflicting datatypes (int, string)\n",
    "        # what should 'none' be instead\n",
    "        entry['ind__base_mins_to_next'] = event_time_dict['next_event_base']\n",
    "        entry['ind__quote_mins_to_next'] = event_time_dict['next_event_quote']            \n",
    "        # looking at trading view and FF, we will use 1hr as a benchmark for 'recent event' effecting the market\n",
    "        # this value is based on how the market acted around recent events, but is also not a perfect time\n",
    "        # maybe an analysis should be done on this to determine best time\n",
    "        try:\n",
    "            if event_time_dict['past_event_base'] <= 60: rec_base = 1\n",
    "            else: rec_base = 0\n",
    "        except: rec_base = 0\n",
    "        try:\n",
    "            if event_time_dict['past_event_quote'] <= 60: rec_quote = 1\n",
    "            else: rec_quote = 0\n",
    "        except: rec_quote = 0\n",
    "        entry['ind__base_recentevent'] = rec_base\n",
    "        entry['ind__quote_recentevent'] = rec_quote\n",
    "        # add on rest of indicators\n",
    "        if base_events != 'empty': entry['ind__base_numevents'] = len([event for event in base_events if \\\n",
    "                                                                      type(event['time_until']) in [float, int]])\n",
    "        else: entry['ind__base_numevents'] = 0\n",
    "        if quote_events != 'empty': entry['ind__quote_numevents'] = len([event for event in quote_events if \\\n",
    "                                                                      type(event['time_until']) in [float, int]])\n",
    "        else: entry['ind__quote_numevents'] = 0\n",
    "        # find IMPACT of upcoming event and most recent event, for both currencies\n",
    "        # also get the forecast of these events, and the actual outcome of past events\n",
    "        # we are going to string together the impact with the actual/forecast        \n",
    "        # upcoming\n",
    "        for cur in ['base','quote']:\n",
    "            if cur=='base': cur_ = base_events\n",
    "            else: cur_ = quote_events\n",
    "            if event_time_dict['next_event_%s' % cur] == 'none': \n",
    "                ev_fore = 'none'\n",
    "                ev_alert = 'none'\n",
    "                ev_sent = 'none'\n",
    "            else:\n",
    "                # check if there are more than 1 events coming up\n",
    "                upcoming = [item for item in cur_ if type(item['time_until']) in [int, float]]\n",
    "                upcoming.sort(key=lambda val:val['time_until'])\n",
    "                # now check upcoming forecast and ff alerts\n",
    "                # pull index 1 from the tuple - index 1 is the forecast prediction, index 0 shows if previous was updated\n",
    "                forecast = [(fore['impact_level'],fore['forecast'][1]) for fore in [event for event in upcoming \\\n",
    "                                                             if event['time_until']==upcoming[0]['time_until']]]\n",
    "                ff_alert = [(alert['impact_level'],str(alert['ff_alert'])) for alert in [event for event in upcoming if event['time_until']==\\\n",
    "                                                            upcoming[0]['time_until']] if alert['ff_alert']==1]\n",
    "                sentiment = [(sent['impact_level'],sent['event_sentiment']) for sent in [event for event in upcoming \\\n",
    "                                                             if event['time_until']==upcoming[0]['time_until']]]                \n",
    "                # compile event indicators\n",
    "                # check event forecast\n",
    "                event_forecast = ['_'.join(forecast[i]) for i in range(len(forecast))]\n",
    "                if len(event_forecast)==1: ev_fore = event_forecast[0]\n",
    "                else:\n",
    "                    if len(np.unique(event_forecast))==1: ev_fore = '_'.join(['multiple',event_forecast[0]])\n",
    "                    else: ev_fore = '__'.join(event_forecast) # double underscore to seperate events\n",
    "                # check ff alerts\n",
    "                if len(ff_alert)==0: ev_alert = 'none'\n",
    "                else: \n",
    "                    event_ffalert = ['_'.join(ff_alert[i]) for i in range(len(ff_alert))]\n",
    "                    if len(event_ffalert)==1: ev_alert = event_ffalert[0]\n",
    "                    else:\n",
    "                        if len(np.unique(event_ffalert))==1: ev_alert = '_'.join(['multiple',event_ffalert[0]])\n",
    "                        else: ev_alert = '__'.join(event_ffalert) # double underscore to seperate events\n",
    "                # check sentiments\n",
    "                event_sentiment = ['_'.join(sentiment[i]) for i in range(len(sentiment))]\n",
    "                if len(event_sentiment)==1: ev_sent = event_sentiment[0]\n",
    "                else:\n",
    "                    if len(np.unique(event_sentiment))==1: ev_sent = '_'.join(['multiple',event_sentiment[0]])\n",
    "                    else: ev_sent = '__'.join(event_sentiment) # double underscore to seperate events\n",
    "            entry['ind__%s_foreimpact' % cur] = ev_fore\n",
    "            entry['ind__%s_ffalert' % cur] = ev_alert\n",
    "            entry['ind__%s_nextevent_sent' % cur] = ev_sent\n",
    "        # past\n",
    "        # need to identify if still in the 'recent event' range - currently an hour\n",
    "        for cur in ['base','quote']:\n",
    "            if cur=='base': cur_ = base_events\n",
    "            else: cur_ = quote_events\n",
    "            if event_time_dict['past_event_%s' % cur] == 'none': \n",
    "                ev_act = 'none'\n",
    "                ev_sent = 'none'\n",
    "            else:\n",
    "                # first check if there was an event in the past hour\n",
    "                # don't need to check like forecast, because 'ind__{cur}_recentevent' indicator will be 0 if 'none' is present\n",
    "                if entry['ind__%s_recentevent'  % cur][0] == 0: ev_act = 'none'\n",
    "                else:\n",
    "                    most_recent = [item for item in cur_ if type(item['time_until']) in [tuple]]\n",
    "                    most_recent.sort(key=lambda val:val['time_until'][1])\n",
    "                    # check if there are more than 1 events coming up\n",
    "                    actual = [(act['impact_level'],act['actual_result']) for act in [event for event in most_recent \\\n",
    "                                                               if event['time_until'][1]==most_recent[0]['time_until'][1]]]\n",
    "                    sentiment = [(sent['impact_level'],sent['event_sentiment']) for sent in [event for event in most_recent \\\n",
    "                                                                 if event['time_until'][1]==most_recent[0]['time_until'][1]]]\n",
    "                    # check acutal\n",
    "                    event_actual = ['_'.join(actual[i]) for i in range(len(actual))]\n",
    "                    if len(event_actual)==1: ev_act = event_actual[0]\n",
    "                    else:\n",
    "                        if len(np.unique(event_actual))==1: ev_act = '_'.join(['multiple',event_actual[0]])\n",
    "                        else: ev_act = '__'.join(event_actual) # double underscore to seperate events\n",
    "                    # check sentiment\n",
    "                    event_sentiment = ['_'.join(sentiment[i]) for i in range(len(sentiment))]\n",
    "                    if len(event_sentiment)==1: ev_sent = event_sentiment[0]\n",
    "                    else:\n",
    "                        if len(np.unique(event_sentiment))==1: ev_sent = '_'.join(['multiple',event_sentiment[0]])\n",
    "                        else: ev_sent = '__'.join(event_sentiment) # double underscore to seperate events\n",
    "            entry['ind__%s_pastimpact' % cur] = ev_act\n",
    "            entry['ind__%s_pastevent_sent' % cur] = ev_sent\n",
    "        print('Done ...                                                                       Time elapsed:',round((time.time() - st), 2),'s')\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        store_entry['entry_'+str(i)] = entry\n",
    "        # save entry as the latest value to predict\n",
    "        entry.to_csv(f'data/predict/to_predict_{SYMBOL}_{pos_time_int}_min_pos.csv')\n",
    "        print('Entry ready for prediction.')\n",
    "        # initialize to_pop list\n",
    "        to_pop = []\n",
    "        for key in store_entry.keys():\n",
    "            ent = store_entry[key]\n",
    "            # check the last 3 data entries just in case of a missed time entry due to delay in program\n",
    "            for check_time in df[-3:].index:\n",
    "                if (check_time - ent.index[0]) / np.timedelta64(1,'m') == pos_time_int:\n",
    "                    result = df.loc[check_time:].Close[0] - ent.Close[0]\n",
    "                    ent['expiration'] = check_time\n",
    "                    ent['exch_rate_dif'] = result\n",
    "                    if result > 0: ent['direction'] = 'up'\n",
    "                    elif result < 0: ent['direction'] = 'down'\n",
    "                    else: ent['direction'] = 'N/A'\n",
    "                    to_path = f'../data/full_data/updated_apicall_data/ext_{SYMBOL}_train_{pos_time_int}_min_pos.csv'\n",
    "                    search_path = '../data/full_data/updated_apicall_data/*'\n",
    "                    check_file = f'../data/full_data/updated_apicall_data\\\\ext_{SYMBOL}_train_{pos_time_int}_min_pos.csv'\n",
    "                    if check_file in glob.glob(search_path):\n",
    "                        training_df = pd.read_csv(to_path, index_col='date')\n",
    "                        t_df = pd.concat([training_df, ent])\n",
    "                        t_df.to_csv(to_path)\n",
    "                        print(SYMBOL,'Training Data shape:',t_df.shape,'\\n')\n",
    "                    else:\n",
    "                        ent.to_csv(to_path)\n",
    "                        print('Dataframe',to_path,'initialized ... \\n')\n",
    "                    to_pop.append(key)\n",
    "                    break\n",
    "        for item in to_pop:\n",
    "            store_entry.pop(item)\n",
    "        \n",
    "        print('RUNTIME:',round(time.time()-st, 2),'seconds\\n')\n",
    "        time.sleep(5)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Datamining\n",
    "[To top^^^](#Table-of-Contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE TO START PROGRAM ON THE EARLY SIDE OF A MINUTE WHEN DATAMINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# %pdb on ##-- use this for debugging\n",
    "\n",
    "def main(SYMBOL='EURJPY', pos_time_int=3): ## EURCAD AUDNZD\n",
    "    create_training_data(SYMBOL=SYMBOL, pos_time_int=pos_time_int)\n",
    "\n",
    "# use this to account for livecharts being down, wait to execute code until it is available\n",
    "initial = datetime.datetime.now()\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    try:\n",
    "        requests.get('http://livecharts.co.uk/')\n",
    "        print('Website up.')\n",
    "        now = datetime.datetime.now()\n",
    "        while now.second>8:\n",
    "            now = datetime.datetime.now()\n",
    "            time.sleep(0.5)\n",
    "        # for other notebooks, uncomment below line\n",
    "        # time.sleep(n) ## n is number of seconds to wait so that we don't overload at 0 second\n",
    "        print('STARTING ...\\n')\n",
    "        main()\n",
    "    except KeyboardInterrupt: raise\n",
    "    except:\n",
    "        print('Not up yet, waiting one minute.')\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted from full_datamining_framework\n",
    "- new trendline analysis and indicators\n",
    "- surfing sensitivity adjusted to 85% - was 75%\n",
    "\n",
    "### adjustments made for new api call, twitter sentiment, and surfing indicator\n",
    "- new api calls will be more accurate in the database\n",
    "- added feature for number of tweets in past minute\n",
    "- adjusted the surfing indicator feature -- made it a percent of where the close price is vs the range of the bollinger band\n",
    "    - made it rolling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
